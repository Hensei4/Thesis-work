{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdTdlDPFW0eS"
      },
      "outputs": [],
      "source": [
        "# 1️⃣ Install the CMU Multimodal SDK\n",
        "!git clone https://github.com/ecfm/CMU-MultimodalSDK.git\n",
        "%cd CMU-MultimodalSDK\n",
        "!pip install .\n",
        "!pip install validators\n",
        "\n",
        "# 2️⃣ Import required modules\n",
        "from mmsdk import mmdatasdk as md\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# 3️⃣ Define the paths to your MOSEI .csd files\n",
        "mosei_csd_files = {\n",
        "    \"CMU_MOSEI_TimestampedWordVectors\": \"/content/drive/MyDrive/EmotionFusion/MOSEI_features/CMU_MOSEI_TimestampedWordVectors.csd\",\n",
        "    \"CMU_MOSEI_COVAREP\": \"/content/drive/MyDrive/EmotionFusion/MOSEI_features/CMU_MOSEI_COVAREP.csd\",\n",
        "    \"CMU_MOSEI_VisualFacet42\": \"/content/drive/MyDrive/EmotionFusion/MOSEI_features/CMU_MOSEI_VisualFacet42.csd\",\n",
        "    \"CMU_MOSEI_Labels\": \"/content/drive/MyDrive/EmotionFusion/MOSEI_features/CMU_MOSEI_Labels.csd\"\n",
        "}\n",
        "\n",
        "#  Load the dataset\n",
        "dataset = md.mmdataset(mosei_csd_files)\n",
        "\n",
        "#  Inspect available modalities and sample keys\n",
        "print(\"Modalities loaded:\", dataset.keys())\n",
        "print(\"Sample utterance keys (first 5):\", list(dataset[\"CMU_MOSEI_Labels\"].keys())[:5])\n",
        "\n",
        "#  Access features for a single utterance\n",
        "sample_key = list(dataset[\"CMU_MOSEI_Labels\"].keys())[0]  # example: pick first utterance\n",
        "text_features = dataset[\"CMU_MOSEI_TimestampedWordVectors\"][sample_key]['features']\n",
        "audio_features = dataset[\"CMU_MOSEI_COVAREP\"][sample_key]['features']\n",
        "visual_features = dataset[\"CMU_MOSEI_VisualFacet42\"][sample_key]['features']\n",
        "label = dataset[\"CMU_MOSEI_Labels\"][sample_key]['features']\n",
        "\n",
        "sample_key = list(dataset['CMU_MOSEI_Labels'].keys())[0]\n",
        "text_features = dataset['CMU_MOSEI_TimestampedWordVectors'][sample_key]['features']\n",
        "audio_features = dataset['CMU_MOSEI_COVAREP'][sample_key]['features']\n",
        "visual_features = dataset['CMU_MOSEI_VisualFacet42'][sample_key]['features']\n",
        "\n",
        "# ✅ get label data directly from dataset.data dict\n",
        "label_data = dataset['CMU_MOSEI_Labels'].data[sample_key]['features']\n",
        "\n",
        "sentiment_score = label_data[0]\n",
        "emotion_classes = label_data[1:]\n",
        "\n",
        "\n",
        "print(f\"\\nFeatures for sample {sample_key}:\")\n",
        "print(f\"  Text: shape {text_features.shape}\")\n",
        "print(f\"  Audio: shape {audio_features.shape}\")\n",
        "print(f\"  Visual: shape {visual_features.shape}\")\n",
        "print(f\"  Label sentiment: {sentiment_score}\")\n",
        "print(f\"  Label emotions: {emotion_classes}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QaNHP49Rykbg"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"⏳ Aligning all modalities to text...\")\n",
        "dataset.align('CMU_MOSEI_TimestampedWordVectors')\n",
        "print(\"✅ Alignment complete.\")\n",
        "\n",
        "\n",
        "sample_key = list(dataset['CMU_MOSEI_Labels'].keys())[0]\n",
        "if sample_key in dataset['CMU_MOSEI_TimestampedWordVectors']:\n",
        "    print(f\"✅ Sample {sample_key} verified in aligned dataset.\")\n",
        "else:\n",
        "    print(\"⚠️ Sample not found after alignment.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o82YfZE3Bh3u"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/EmotionFusion/Aligned_MOSEI.pkl\"\n",
        "\n",
        "with open(save_path, 'wb') as f:\n",
        "    pickle.dump(dataset, f)\n",
        "\n",
        "print(f\"✅ Saved aligned dataset to {save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03FnF8TnLzMe"
      },
      "outputs": [],
      "source": [
        "\n",
        "!git clone https://github.com/ecfm/CMU-MultimodalSDK.git\n",
        "%cd CMU-MultimodalSDK\n",
        "!pip install .\n",
        "!pip install validators\n",
        "\n",
        "\n",
        "from mmsdk import mmdatasdk as md\n",
        "import pickle\n",
        "\n",
        "with open(\"/content/drive/MyDrive/EmotionFusion/Aligned_MOSEI.pkl\", \"rb\") as f:\n",
        "    dataset = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET4UwWKi1ROr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!git clone https://github.com/ecfm/CMU-MultimodalSDK.git\n",
        "%cd CMU-MultimodalSDK\n",
        "!pip install .\n",
        "!pip install validators\n",
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rRCC51fLS80"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pickle, torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer, Callback\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from torchmetrics.classification import MultilabelF1Score, MultilabelAccuracy\n",
        "\n",
        "def safe_tensor(arr, clamp=10_000.0):\n",
        "    t = torch.tensor(arr, dtype=torch.float32)\n",
        "    t = torch.nan_to_num(t, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return torch.clamp(t, -clamp, clamp)\n",
        "\n",
        "def z_norm(seq, eps=1e-8):\n",
        "    m, s = seq.mean(0, keepdim=True), seq.std(0, keepdim=True, unbiased=False)\n",
        "    return (seq - m) / (s + eps)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/EmotionFusion/Aligned_MOSEI.pkl\", \"rb\") as f:\n",
        "    dataset = pickle.load(f)\n",
        "print(\" Dataset loaded.\")\n",
        "\n",
        "from mmsdk.mmdatasdk.dataset.standard_datasets.CMU_MOSEI.cmu_mosei_std_folds import \\\n",
        "    standard_train_fold, standard_valid_fold\n",
        "def vid_id(k): return k.split('[')[0].lstrip('-')\n",
        "\n",
        "keys = {name: dataset[name].keys() for name in\n",
        "        ['CMU_MOSEI_TimestampedWordVectors','CMU_MOSEI_COVAREP',\n",
        "         'CMU_MOSEI_VisualFacet42','CMU_MOSEI_Labels']}\n",
        "avail = set.intersection(*map(set, keys.values()))\n",
        "\n",
        "train_ids = [k for k in keys['CMU_MOSEI_Labels']\n",
        "             if vid_id(k) in standard_train_fold and k in avail]\n",
        "val_ids   = [k for k in keys['CMU_MOSEI_Labels']\n",
        "             if vid_id(k) in standard_valid_fold and k in avail]\n",
        "print(f\" Filtered splits: {len(train_ids)} train • {len(val_ids)} val\")\n",
        "\n",
        "class MOSEI(Dataset):\n",
        "    def __init__(self, ids): self.ids = ids\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        k = self.ids[idx]; d = dataset\n",
        "        x_t = z_norm(safe_tensor(d['CMU_MOSEI_TimestampedWordVectors'][k]['features']))\n",
        "        x_a = z_norm(safe_tensor(d['CMU_MOSEI_COVAREP'][k]['features']))\n",
        "        x_v = z_norm(safe_tensor(d['CMU_MOSEI_VisualFacet42'][k]['features']))\n",
        "        y   = torch.tensor(d['CMU_MOSEI_Labels'][k]['features'], dtype=torch.float32).mean(0)\n",
        "        y   = torch.nan_to_num(y).clamp(0,1)\n",
        "        return x_t, x_a, x_v, y\n",
        "\n",
        "def collate(batch):\n",
        "    t,a,v,y = zip(*batch)\n",
        "    tlen = torch.tensor([x.size(0) for x in t])\n",
        "    alen = torch.tensor([x.size(0) for x in a])\n",
        "    vlen = torch.tensor([x.size(0) for x in v])\n",
        "    return (nn.utils.rnn.pad_sequence(t,batch_first=True), tlen,\n",
        "            nn.utils.rnn.pad_sequence(a,batch_first=True), alen,\n",
        "            nn.utils.rnn.pad_sequence(v,batch_first=True), vlen,\n",
        "            torch.stack(y))\n",
        "\n",
        "train_loader = DataLoader(MOSEI(train_ids),32,True, collate_fn=collate,num_workers=4,pin_memory=True)\n",
        "val_loader   = DataLoader(MOSEI(val_ids)  ,32,False,collate_fn=collate,num_workers=4,pin_memory=True)\n",
        "\n",
        "bin_counts = torch.zeros(7)\n",
        "for _,_,_,y in MOSEI(train_ids):\n",
        "    bin_counts += (y>0).float()\n",
        "pos_weight = (len(train_ids)-bin_counts)/(bin_counts+1e-6)\n",
        "print(f\"  pos_weight = {pos_weight.tolist()}\")\n",
        "\n",
        "class XModal(pl.LightningModule):\n",
        "    def __init__(self, pos_weight):\n",
        "        super().__init__()\n",
        "        self.text_rnn   = nn.GRU(300,256,batch_first=True)\n",
        "        self.audio_rnn  = nn.GRU(74 ,128,batch_first=True)\n",
        "        self.visual_rnn = nn.GRU(35 ,128,batch_first=True)\n",
        "        self.fc = nn.Linear(256+128+128,7)\n",
        "        self.loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "        self.thresh = 0.3\n",
        "        self._micro = MultilabelF1Score(7,threshold=self.thresh,average='micro')\n",
        "        self._macro = MultilabelF1Score(7,threshold=self.thresh,average='macro')\n",
        "        self._acc   = MultilabelAccuracy(7,threshold=self.thresh)\n",
        "\n",
        "    @staticmethod\n",
        "    def _last(rnn,x,l): return rnn(pack_padded_sequence(x,l.cpu(),batch_first=True,enforce_sorted=False))[1].squeeze(0)\n",
        "\n",
        "    def forward(self,t,tlen,a,alen,v,vlen):\n",
        "        h = torch.cat([self._last(self.text_rnn,t,tlen),\n",
        "                       self._last(self.audio_rnn,a,alen),\n",
        "                       self._last(self.visual_rnn,v,vlen)],-1)\n",
        "        return self.fc(h)\n",
        "\n",
        "    def _common_step(self,b):\n",
        "        t,tlen,a,alen,v,vlen,y = b\n",
        "        logit = self(t,tlen,a,alen,v,vlen); loss=self.loss(logit,y)\n",
        "        return loss, logit, (y>0).int()\n",
        "\n",
        "\n",
        "    def training_step(self,b,_):\n",
        "        loss,_,_ = self._common_step(b)\n",
        "        self.log(\"train_loss\",loss,prog_bar=True); return loss\n",
        "\n",
        "\n",
        "    def validation_step(self,b,_):\n",
        "        loss,logit,yt = self._common_step(b)\n",
        "        self.log(\"val_loss\",loss,prog_bar=True)\n",
        "        pred = (torch.sigmoid(logit)>self.thresh).int()\n",
        "        self._micro.update(pred,yt); self._macro.update(pred,yt); self._acc.update(pred,yt)\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        micro = self._micro.compute(); macro = self._macro.compute(); acc = self._acc.compute()\n",
        "        self.log_dict({\"val_f1_micro\":micro,\"val_f1_macro\":macro,\"val_acc\":acc},prog_bar=True)\n",
        "        self._micro.reset(); self._macro.reset(); self._acc.reset()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.parameters(),lr=1e-4,amsgrad=True)\n",
        "\n",
        "class SweepThreshold(Callback):\n",
        "    def __init__(self, every=1): self.every=every\n",
        "    def on_validation_epoch_end(self,tr,pl_module):\n",
        "        if (tr.current_epoch+1)%self.every: return\n",
        "        logits, targets = [], []\n",
        "        pl_module.eval(); dev=pl_module.device\n",
        "        with torch.no_grad():\n",
        "            for b in val_loader:\n",
        "                b=[x.to(dev) for x in b]; logits.append(pl_module(*b[:-1]).cpu()); targets.append((b[-1]>0).int().cpu())\n",
        "        logits, targets = torch.cat(logits), torch.cat(targets)\n",
        "        best_t,best=pl_module.thresh,0.0\n",
        "        for t in torch.linspace(0.05,0.5,10):\n",
        "            f1 = MultilabelF1Score(7,threshold=0.5,average='micro')((torch.sigmoid(logits)>t).int(),targets)\n",
        "            if f1>best: best,best_t=f1.item(),t.item()\n",
        "        pl_module.thresh=best_t\n",
        "        print(f\"🔄  new decision threshold = {best_t:.2f} (micro-F1={best:.3f})\")\n",
        "\n",
        "\n",
        "ckpt = ModelCheckpoint(\"/content/drive/MyDrive/EmotionFusion/checkpoints\",\n",
        "                       filename=\"best-{epoch:02d}-{val_f1_micro:.3f}\",\n",
        "                       monitor=\"val_f1_micro\",mode=\"max\",save_top_k=1)\n",
        "early = EarlyStopping(monitor=\"val_f1_micro\",mode=\"max\",patience=5,verbose=True)\n",
        "\n",
        "trainer = Trainer(max_epochs=30,accelerator=\"auto\",\n",
        "                  devices=1 if torch.cuda.is_available() else None,\n",
        "                  gradient_clip_val=1.0,\n",
        "                  callbacks=[ckpt,early,SweepThreshold()])\n",
        "\n",
        "model = XModal(pos_weight)\n",
        "trainer.fit(model,train_loader,val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ItELWaieYaa"
      },
      "outputs": [],
      "source": [
        "import pickle, torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from torchmetrics.classification import MultilabelF1Score, MultilabelAccuracy\n",
        "\n",
        "\n",
        "def safe_tensor(arr, clamp=10_000.0):\n",
        "    t = torch.tensor(arr, dtype=torch.float32)\n",
        "    t = torch.nan_to_num(t, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return torch.clamp(t, -clamp, clamp)\n",
        "\n",
        "def z_norm(seq, eps=1e-8):\n",
        "    m, s = seq.mean(0, keepdim=True), seq.std(0, keepdim=True, unbiased=False)\n",
        "    return (seq - m) / (s + eps)\n",
        "\n",
        "\n",
        "with open(\"/content/drive/MyDrive/EmotionFusion/Aligned_MOSEI.pkl\", \"rb\") as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "from mmsdk.mmdatasdk.dataset.standard_datasets.CMU_MOSEI.cmu_mosei_std_folds import standard_test_fold\n",
        "def vid_id(k): return k.split('[')[0].lstrip('-')\n",
        "\n",
        "keys = {name: dataset[name].keys() for name in\n",
        "        ['CMU_MOSEI_TimestampedWordVectors','CMU_MOSEI_COVAREP',\n",
        "         'CMU_MOSEI_VisualFacet42','CMU_MOSEI_Labels']}\n",
        "avail = set.intersection(*map(set, keys.values()))\n",
        "test_ids = [k for k in keys['CMU_MOSEI_Labels']\n",
        "            if vid_id(k) in standard_test_fold and k in avail]\n",
        "print(f\"✅ Test split size: {len(test_ids)}\")\n",
        "\n",
        "\n",
        "class MOSEITest(Dataset):\n",
        "    def __init__(self, ids): self.ids=ids\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        k = self.ids[idx]; d = dataset\n",
        "        t = z_norm(safe_tensor(d['CMU_MOSEI_TimestampedWordVectors'][k]['features']))\n",
        "        a = z_norm(safe_tensor(d['CMU_MOSEI_COVAREP'][k]['features']))\n",
        "        v = z_norm(safe_tensor(d['CMU_MOSEI_VisualFacet42'][k]['features']))\n",
        "        y = torch.tensor(d['CMU_MOSEI_Labels'][k]['features'], dtype=torch.float32).mean(0)\n",
        "        y = torch.nan_to_num(y).clamp(0,1)\n",
        "        return t,a,v,y\n",
        "\n",
        "def collate(batch):\n",
        "    t,a,v,y=zip(*batch)\n",
        "    tlen=torch.tensor([x.size(0) for x in t]); alen=torch.tensor([x.size(0) for x in a])\n",
        "    vlen=torch.tensor([x.size(0) for x in v])\n",
        "    return (nn.utils.rnn.pad_sequence(t,batch_first=True), tlen,\n",
        "            nn.utils.rnn.pad_sequence(a,batch_first=True), alen,\n",
        "            nn.utils.rnn.pad_sequence(v,batch_first=True), vlen,\n",
        "            torch.stack(y))\n",
        "\n",
        "test_loader = DataLoader(MOSEITest(test_ids), batch_size=32, shuffle=False,\n",
        "                         collate_fn=collate, num_workers=4, pin_memory=True)\n",
        "\n",
        "\n",
        "class XModal(pl.LightningModule):\n",
        "    def __init__(self, pos_weight=torch.ones(7)):\n",
        "        super().__init__()\n",
        "        self.text_rnn = nn.GRU(300,256,batch_first=True)\n",
        "        self.audio_rnn= nn.GRU(74 ,128,batch_first=True)\n",
        "        self.visual_rnn=nn.GRU(35 ,128,batch_first=True)\n",
        "        self.fc = nn.Linear(256+128+128,7)\n",
        "        self.loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "        self.thresh = 0.3                                    # will be overwritten by checkpoint\n",
        "    @staticmethod\n",
        "    def _last(rnn,x,l): return rnn(pack_padded_sequence(x,l.cpu(),batch_first=True,enforce_sorted=False))[1].squeeze(0)\n",
        "    def forward(self,t,tlen,a,alen,v,vlen):\n",
        "        h=torch.cat([self._last(self.text_rnn,t,tlen),\n",
        "                     self._last(self.audio_rnn,a,alen),\n",
        "                     self._last(self.visual_rnn,v,vlen)],-1)\n",
        "        return self.fc(h)\n",
        "\n",
        "\n",
        "ckpt_path = \"/content/drive/MyDrive/EmotionFusion/checkpoints/best-epoch=12-val_f1_micro=0.459.ckpt\"\n",
        "model = XModal.load_from_checkpoint(ckpt_path)\n",
        "print(f\"🔑  Loaded checkpoint from {ckpt_path}\")\n",
        "print(f\"🔎  Decision threshold stored in checkpoint: {model.thresh:.2f}\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device).eval()\n",
        "\n",
        "micro = MultilabelF1Score(7,threshold=model.thresh,average='micro').to(device)\n",
        "macro = MultilabelF1Score(7,threshold=model.thresh,average='macro').to(device)\n",
        "acc   = MultilabelAccuracy(7,threshold=model.thresh).to(device)\n",
        "per_class_f1 = MultilabelF1Score(7,threshold=model.thresh,average=None).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for b in test_loader:\n",
        "        b = [x.to(device) for x in b]\n",
        "        logits = model(*b[:-1])\n",
        "        preds  = (torch.sigmoid(logits)>model.thresh).int()\n",
        "        target = (b[-1]>0).int()\n",
        "        micro.update(preds,target); macro.update(preds,target)\n",
        "        acc.update(preds,target);   per_class_f1.update(preds,target)\n",
        "\n",
        "print(f\"\\n===== Test-set results =====\")\n",
        "print(f\"Micro-F1  : {micro.compute():.3f}\")\n",
        "print(f\"Macro-F1  : {macro.compute():.3f}\")\n",
        "print(f\"Accuracy  : {acc.compute():.3f}\")\n",
        "print(f\"Per-class F1 [neutral, happy, sad, anger, fear, disgust, surprise]:\\n  {per_class_f1.compute().cpu().numpy().round(3)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8I3lreb7oWm"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "echo \"🚀  Fast reinstall using wheels only\"\n",
        "pip -q uninstall -y torch torchvision torchaudio triton pytorch-lightning scipy numpy pandas gensim openai-whisper || true\n",
        "\n",
        "pip -q install numpy==1.26.4 scipy==1.11.4 pandas==2.2.2 tqdm           # will pull wheels\n",
        "pip -q install torch==2.1.2+cu121 torchvision==0.16.2+cu121 torchaudio==2.1.2+cu121 \\\n",
        "               --index-url https://download.pytorch.org/whl/cu121\n",
        "pip -q install pytorch-lightning==2.1.3 triton==2.0.0 openai-whisper==20230314 gensim==4.3.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzMPU851Y4ps"
      },
      "outputs": [],
      "source": [
        "!pip install whisper\n",
        "!pip install gensim\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITrB2NbYWihs"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -e\n",
        "echo \"🧹 uninstall any orphan whisper\"\n",
        "pip -q uninstall -y whisper || true\n",
        "\n",
        "echo \"📦 install CPU-only wheels (Python 3.11 compatible)…\"\n",
        "pip -q install --upgrade pip\n",
        "pip -q install \\\n",
        "      numpy==1.26.4 scipy==1.11.4 pandas==2.2.2 tqdm scikit-learn \\\n",
        "      torch==2.2.1 torchaudio==2.2.1 torchvision==0.17.1           \\\n",
        "      git+https://github.com/openai/whisper.git                    \\\n",
        "      gensim==4.3.2 py-feat==0.5.0 opencv-python-headless==4.9.0.80\n",
        "\n",
        "echo \"✅  all deps ready\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qFl1yL3cdth"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import csv, pickle, numpy as np, cv2, subprocess, tempfile\n",
        "import torch, torchaudio, torchvision, whisper, gensim.downloader as api\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.decomposition import PCA\n",
        "import torchvision.transforms as T\n",
        "from google.colab import drive\n",
        "\n",
        "# 0. Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "root    = Path(\"/content/drive/MyDrive/movie_clips\")   # mp4 + labels.csv now on Drive\n",
        "lab_csv = root / \"labels.csv\"\n",
        "assert lab_csv.exists(), \"labels.csv missing!\"\n",
        "\n",
        "with lab_csv.open() as f:\n",
        "    rdr = csv.DictReader(f)\n",
        "    emo_cols = rdr.fieldnames[1:]\n",
        "    labels = {row[\"clip_id\"]: np.array([int(row[c]) for c in emo_cols], np.float32)\n",
        "              for row in rdr}\n",
        "\n",
        "\n",
        "whisp = whisper.load_model(\"base\")                      # CPU is fine\n",
        "w2v   = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "mel74 = torchaudio.transforms.MelSpectrogram(sample_rate=16_000, n_mels=74)\n",
        "\n",
        "resnet_full = torchvision.models.resnet18(weights=\"DEFAULT\").eval()\n",
        "feature_net = torch.nn.Sequential(*list(resnet_full.children())[:-1]).eval()  # outputs (B,512,1,1)\n",
        "to224 = T.Compose([T.ToPILImage(), T.Resize(224), T.ToTensor()])\n",
        "\n",
        "\n",
        "pca35 = PCA(35).fit(np.random.randn(400, 512))\n",
        "\n",
        "def text_vec(words):\n",
        "    return np.stack([w2v[w] if w in w2v else np.zeros(300, np.float32) for w in words]) \\\n",
        "           if words else np.zeros((1, 300), np.float32)\n",
        "\n",
        "def audio_vec(mp4_path):\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".wav\") as tmp:\n",
        "        subprocess.run(\n",
        "            [\"ffmpeg\", \"-loglevel\", \"error\", \"-y\", \"-i\", mp4_path,\n",
        "             \"-ar\", \"16000\", \"-ac\", \"1\", tmp.name]\n",
        "        )\n",
        "        sig, _ = torchaudio.load(tmp.name)\n",
        "    return mel74(sig)[0].mean(-1, keepdim=True).T.numpy()        # (1,74)\n",
        "\n",
        "def visual_seq(mp4_path):\n",
        "    cap = cv2.VideoCapture(str(mp4_path))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 25\n",
        "    step = max(int(fps), 1)\n",
        "    frames, f = [], 0\n",
        "    with torch.no_grad():\n",
        "        while cap.isOpened():\n",
        "            ok, frame = cap.read()\n",
        "            if not ok:\n",
        "                break\n",
        "            if f % step == 0:\n",
        "                x = to224(frame[:, :, ::-1]).unsqueeze(0)        # (1,3,224,224)\n",
        "                vec = feature_net(x).squeeze().numpy()          # (512,)\n",
        "                frames.append(pca35.transform(vec[None])[0])    # (35,)\n",
        "            f += 1\n",
        "    cap.release()\n",
        "    return np.stack(frames) if frames else np.zeros((1, 35), np.float32)\n",
        "\n",
        "\n",
        "out = {k:{} for k in [\"CMU_MOSEI_TimestampedWordVectors\",\n",
        "                      \"CMU_MOSEI_COVAREP\",\n",
        "                      \"CMU_MOSEI_VisualFacet42\",\n",
        "                      \"CMU_MOSEI_Labels\"]}\n",
        "\n",
        "for mp4 in tqdm(root.glob(\"*.mp4\"), desc=\"extracting\"):\n",
        "    cid = mp4.stem\n",
        "    # --- text ---\n",
        "    seg   = whisp.transcribe(str(mp4), language=\"en\", word_timestamps=True, verbose=False)\n",
        "    words = [w[\"word\"].lower().strip(\".,?!\") for s in seg[\"segments\"] for w in s[\"words\"]]\n",
        "    out[\"CMU_MOSEI_TimestampedWordVectors\"][cid] = {\"features\": text_vec(words)}\n",
        "    # --- audio ---\n",
        "    out[\"CMU_MOSEI_COVAREP\"][cid] = {\"features\": audio_vec(str(mp4))}\n",
        "    # --- visual ---\n",
        "    out[\"CMU_MOSEI_VisualFacet42\"][cid] = {\"features\": visual_seq(mp4)}\n",
        "    # --- label ---\n",
        "    out[\"CMU_MOSEI_Labels\"][cid] = {\"features\": labels[cid]}\n",
        "\n",
        "# ---------- save to Drive ----------\n",
        "save_path = \"/content/drive/MyDrive/movie_clips_simple.pkl\"\n",
        "with open(save_path, \"wb\") as f:\n",
        "    pickle.dump(out, f)\n",
        "\n",
        "print(f\"🎉  Saved movie_clips_simple.pkl with {len(out['CMU_MOSEI_Labels'])} clips → {save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuQ2zfych-4m"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import pickle, torch, torch.nn as nn, pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from torchmetrics.classification import MultilabelF1Score, MultilabelAccuracy\n",
        "\n",
        "\n",
        "with open(\"/content/movie_clips_simple.pkl\", \"rb\") as f:\n",
        "    movie_data = pickle.load(f)\n",
        "\n",
        "clip_ids = list(movie_data[\"CMU_MOSEI_Labels\"])\n",
        "print(\"🔢 clips:\", len(clip_ids))\n",
        "\n",
        "\n",
        "class MovieDS(Dataset):\n",
        "    def __init__(self, ids): self.ids = ids\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        k   = self.ids[idx]; d = movie_data\n",
        "        txt = torch.tensor(d[\"CMU_MOSEI_TimestampedWordVectors\"][k][\"features\"],\n",
        "                           dtype=torch.float32)\n",
        "        aud = torch.tensor(d[\"CMU_MOSEI_COVAREP\"][k][\"features\"],\n",
        "                           dtype=torch.float32)\n",
        "        vis = torch.tensor(d[\"CMU_MOSEI_VisualFacet42\"][k][\"features\"],\n",
        "                           dtype=torch.float32)\n",
        "        lab = torch.tensor(d[\"CMU_MOSEI_Labels\"][k][\"features\"],\n",
        "                           dtype=torch.float32)\n",
        "        return txt, aud, vis, lab\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    t,a,v,y = zip(*batch)\n",
        "    tlen = torch.tensor([x.size(0) for x in t])\n",
        "    alen = torch.tensor([x.size(0) for x in a])\n",
        "    vlen = torch.tensor([x.size(0) for x in v])\n",
        "    return (pad_sequence(t, True), tlen,\n",
        "            pad_sequence(a, True), alen,\n",
        "            pad_sequence(v, True), vlen,\n",
        "            torch.stack(y))\n",
        "\n",
        "loader = DataLoader(MovieDS(clip_ids), batch_size=32, shuffle=False, collate_fn=collate)\n",
        "\n",
        "class XModal(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.text_rnn   = nn.GRU(300, 256, batch_first=True)\n",
        "        self.audio_rnn  = nn.GRU( 74, 128, batch_first=True)\n",
        "        self.visual_rnn = nn.GRU( 35, 128, batch_first=True)\n",
        "        self.fc = nn.Linear(256+128+128, 7)\n",
        "        self.thresh = 0.30                      # will be overwritten by ckpt if stored\n",
        "\n",
        "    def _last(self, rnn, x, lengths):\n",
        "        _, h_n = rnn(pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False))\n",
        "        return h_n.squeeze(0)                   # (B, hidden)\n",
        "\n",
        "    def forward(self, t, tlen, a, alen, v, vlen):\n",
        "        h = torch.cat([self._last(self.text_rnn,  t, tlen),\n",
        "                       self._last(self.audio_rnn, a, alen),\n",
        "                       self._last(self.visual_rnn,v, vlen)], dim=-1)\n",
        "        return self.fc(h)\n",
        "\n",
        "ckpt_path = \"/content/drive/MyDrive/EmotionFusion/checkpoints/best-epoch=12-val_f1_micro=0.459.ckpt\"\n",
        "model = XModal.load_from_checkpoint(ckpt_path, strict=False).eval()\n",
        "device = \"cpu\"                                  # stay on CPU\n",
        "model.to(device)\n",
        "print(f\"🔑  Loaded checkpoint: {ckpt_path}\")\n",
        "print(f\"🔎  Decision threshold: {model.thresh:.2f}\")\n",
        "\n",
        "\n",
        "micro = MultilabelF1Score(num_labels=7, threshold=model.thresh, average=\"micro\").to(device)\n",
        "macro = MultilabelF1Score(num_labels=7, threshold=model.thresh, average=\"macro\").to(device)\n",
        "acc   = MultilabelAccuracy(num_labels=7, threshold=model.thresh).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in loader:\n",
        "        batch = [x.to(device) for x in batch]\n",
        "        logits = model(*batch[:-1])\n",
        "        preds  = (torch.sigmoid(logits) > model.thresh).int()\n",
        "        target = (batch[-1] > 0).int()\n",
        "        micro.update(preds, target)\n",
        "        macro.update(preds, target)\n",
        "        acc.update(preds, target)\n",
        "\n",
        "print(\"\\n=====  Results on 28 movie clips  =====\")\n",
        "print(\"Micro-F1 :\", micro.compute().item())\n",
        "print(\"Macro-F1 :\", macro.compute().item())\n",
        "print(\"Accuracy :\", acc.compute().item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n49B3d6TsE5v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import pickle\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "import opensmile\n",
        "import whisper\n",
        "import gensim.downloader as api\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# 1. Configuration\n",
        "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DRIVE_ROOT   = \"/content/drive/MyDrive\"\n",
        "CAER_ROOT    = f\"{DRIVE_ROOT}/CAER\"\n",
        "OUT_PKL_PATH = f\"{DRIVE_ROOT}/caer_aligned.pkl\"\n",
        "EMOTIONS     = ['Anger', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
        "CAER2MOSEI   = [2, 4, 5, 0, 6, 1, 3]\n",
        "\n",
        "# 2. Load models once\n",
        "print(f\"⚙ Device: {DEVICE}\")\n",
        "whisper_model = whisper.load_model(\"base\").to(DEVICE)\n",
        "smile         = opensmile.Smile(feature_set=opensmile.FeatureSet.ComParE_2016,\n",
        "                               feature_level=opensmile.FeatureLevel.LowLevelDescriptors)\n",
        "resnet        = resnet50(weights=ResNet50_Weights.DEFAULT).to(DEVICE).eval()\n",
        "resnet.fc     = torch.nn.Identity()\n",
        "glove         = api.load(\"glove-wiki-gigaword-300\")\n",
        "\n",
        "img_tf = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406],\n",
        "                [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# 3. Extraction helpers, each returns (True, features) or (False, exception)\n",
        "@torch.no_grad()\n",
        "def extract_visual(path, every_n=10):\n",
        "    try:\n",
        "        cap, feats, idx = cv2.VideoCapture(path), [], 0\n",
        "        while True:\n",
        "            ok, frame = cap.read()\n",
        "            if not ok:\n",
        "                break\n",
        "            if idx % every_n == 0:\n",
        "                x = img_tf(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\\\n",
        "                      .unsqueeze(0).to(DEVICE)\n",
        "                feats.append(resnet(x).cpu())\n",
        "            idx += 1\n",
        "        cap.release()\n",
        "        if not feats:\n",
        "            raise RuntimeError(\"No frames extracted\")\n",
        "        return True, torch.vstack(feats)\n",
        "    except Exception as e:\n",
        "        return False, e\n",
        "\n",
        "def extract_audio(path):\n",
        "    try:\n",
        "        wav, sr = torchaudio.load(path)\n",
        "        if sr != 16000:\n",
        "            wav = torchaudio.functional.resample(wav, sr, 16000)\n",
        "        df = smile.process_signal(wav.squeeze().numpy(), 16000)\n",
        "        if df.empty:\n",
        "            raise RuntimeError(\"Empty audio features\")\n",
        "        return True, torch.tensor(df.values, dtype=torch.float32)\n",
        "    except Exception as e:\n",
        "        return False, e\n",
        "\n",
        "def extract_text(path):\n",
        "    try:\n",
        "        res    = whisper_model.transcribe(path, language=\"en\", fp16=(DEVICE==\"cuda\"))\n",
        "        tokens = res[\"text\"].lower().split()\n",
        "        vecs   = [glove[w] for w in tokens if w in glove]\n",
        "        if not vecs:\n",
        "            raise RuntimeError(\"No valid words in transcription\")\n",
        "        return True, np.mean(vecs, axis=0).astype(np.float32)\n",
        "    except Exception as e:\n",
        "        return False, e\n",
        "\n",
        "# 4. Main processing loop\n",
        "out = {\n",
        "    \"CMU_MOSEI_TimestampedWordVectors\": {},\n",
        "    \"CMU_MOSEI_COVAREP\": {},\n",
        "    \"CMU_MOSEI_VisualFacet42\": {},\n",
        "    \"CMU_MOSEI_Labels\": {},\n",
        "}\n",
        "failures      = defaultdict(list)\n",
        "emotion_counts = Counter()\n",
        "total_clips    = 0\n",
        "\n",
        "for split in [\"train\", \"validation\", \"test\"]:\n",
        "    for emo in EMOTIONS:\n",
        "        files = glob.glob(f\"{CAER_ROOT}/{split}/{emo}/*.avi\")\n",
        "        print(f\"📂 {split}/{emo}: {len(files)} videos\")\n",
        "        for vid in tqdm(files, desc=f\"{split}/{emo}\", ncols=80):\n",
        "            total_clips += 1\n",
        "            cid = Path(vid).stem\n",
        "\n",
        "            # Run each extractor\n",
        "            ok_t, txt = extract_text(vid)\n",
        "            ok_a, aud = extract_audio(vid)\n",
        "            ok_v, vis = extract_visual(vid)\n",
        "\n",
        "            # Record successes\n",
        "            if ok_t:\n",
        "                out[\"CMU_MOSEI_TimestampedWordVectors\"][cid] = {\"features\": torch.tensor(txt)}\n",
        "            else:\n",
        "                failures[cid].append(f\"text: {txt}\")\n",
        "\n",
        "            if ok_a:\n",
        "                out[\"CMU_MOSEI_COVAREP\"][cid] = {\"features\": aud}\n",
        "            else:\n",
        "                failures[cid].append(f\"audio: {aud}\")\n",
        "\n",
        "            if ok_v:\n",
        "                out[\"CMU_MOSEI_VisualFacet42\"][cid] = {\"features\": vis}\n",
        "            else:\n",
        "                failures[cid].append(f\"vision: {vis}\")\n",
        "\n",
        "            # Always store label (we know the emotion)\n",
        "            oh = np.zeros(len(EMOTIONS), np.float32)\n",
        "            oh[CAER2MOSEI[EMOTIONS.index(emo)]] = 1\n",
        "            out[\"CMU_MOSEI_Labels\"][cid] = {\"features\": oh}\n",
        "\n",
        "            # Count if **all three** modalities succeeded\n",
        "            if ok_t and ok_a and ok_v:\n",
        "                emotion_counts[emo] += 1\n",
        "\n",
        "# 5. Report summary\n",
        "print(f\"\\n✅ Processed {total_clips} clips.\")\n",
        "print(\"✅ Fully-successful clips per emotion:\")\n",
        "for emo, cnt in emotion_counts.items():\n",
        "    print(f\"  - {emo}: {cnt}\")\n",
        "\n",
        "print(f\"\\n⚠ {len(failures)} clips had at least one failure.\")\n",
        "# Optionally, write failures to a log file for later manual inspection:\n",
        "with open(\"failure_log.txt\", \"w\") as fw:\n",
        "    for cid, errs in failures.items():\n",
        "        fw.write(f\"{cid}: \" + \"; \".join(errs) + \"\\n\")\n",
        "\n",
        "# 6. Save output\n",
        "backup = OUT_PKL_PATH + \".bak\"\n",
        "if Path(OUT_PKL_PATH).exists():\n",
        "    Path(OUT_PKL_PATH).rename(backup)\n",
        "    print(f\"🔄 Backed up existing pickle to {backup}\")\n",
        "with open(OUT_PKL_PATH, \"wb\") as f:\n",
        "    pickle.dump(out, f)\n",
        "print(f\"✅ Saved new pickle → {OUT_PKL_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import pickle\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "import opensmile\n",
        "import whisper\n",
        "import gensim.downloader as api\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "import torchvision.transforms as T\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 2. Configuration\n",
        "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DRIVE_ROOT   = \"/content/drive/MyDrive\"\n",
        "CAER_ROOT    = f\"{DRIVE_ROOT}/CAER\"\n",
        "OUT_PKL_PATH = f\"{DRIVE_ROOT}/caer_aligned_full.pkl\"  # final output\n",
        "\n",
        "EMOTIONS   = ['Anger','Disgust','Fear','Happy','Neutral','Sad','Surprise']\n",
        "CAER2MOSEI = [2,4,5,0,6,1,3]\n",
        "\n",
        "# 3. Prepare empty output\n",
        "out = {\n",
        "    \"CMU_MOSEI_TimestampedWordVectors\": {},\n",
        "    \"CMU_MOSEI_COVAREP\": {},\n",
        "    \"CMU_MOSEI_VisualFacet42\": {},\n",
        "    \"CMU_MOSEI_Labels\": {},\n",
        "}\n",
        "\n",
        "# 4. Load models\n",
        "print(f\"⚙ Device: {DEVICE}\")\n",
        "whisper_model = whisper.load_model(\"base\").to(DEVICE)\n",
        "smile = opensmile.Smile(\n",
        "    feature_set=opensmile.FeatureSet.ComParE_2016,\n",
        "    feature_level=opensmile.FeatureLevel.LowLevelDescriptors\n",
        ")\n",
        "resnet = resnet50(weights=ResNet50_Weights.DEFAULT).to(DEVICE).eval()\n",
        "resnet.fc = torch.nn.Identity()\n",
        "glove = api.load(\"glove-wiki-gigaword-300\")\n",
        "\n",
        "img_tf = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Resize((224,224)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "# 5. Compute zero-pad placeholders\n",
        "TEXT_DIM  = glove.vector_size\n",
        "AUDIO_DIM = smile.process_signal(np.zeros(16000), 16000).values.shape[1]\n",
        "ZERO_TEXT = torch.zeros(TEXT_DIM, dtype=torch.float32)\n",
        "ZERO_AUD  = torch.zeros(AUDIO_DIM, dtype=torch.float32)\n",
        "ZERO_VIS  = torch.zeros(1, 2048, dtype=torch.float32)  # one frame of ResNet features\n",
        "\n",
        "# 6. Extraction helpers\n",
        "@torch.no_grad()\n",
        "def extract_visual(path, every_n=10):\n",
        "    cap, feats, idx = cv2.VideoCapture(path), [], 0\n",
        "    while True:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok: break\n",
        "        if idx % every_n == 0:\n",
        "            x = img_tf(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).unsqueeze(0).to(DEVICE)\n",
        "            feats.append(resnet(x).cpu())\n",
        "        idx += 1\n",
        "    cap.release()\n",
        "    if not feats:\n",
        "        raise RuntimeError(\"No frames extracted\")\n",
        "    return torch.vstack(feats)\n",
        "\n",
        "def extract_audio(path):\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    if sr != 16000:\n",
        "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
        "    df = smile.process_signal(wav.squeeze().numpy(), 16000)\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Empty audio features\")\n",
        "    return torch.tensor(df.values, dtype=torch.float32)\n",
        "\n",
        "def extract_text(path):\n",
        "    outp = whisper_model.transcribe(path, language=\"en\", fp16=(DEVICE==\"cuda\"))\n",
        "    toks = outp[\"text\"].lower().split()\n",
        "    vecs = [glove[w] for w in toks if w in glove]\n",
        "    if not vecs:\n",
        "        raise RuntimeError(\"No valid words\")\n",
        "    return np.mean(vecs, axis=0).astype(np.float32)\n",
        "\n",
        "# 7. Main loop: process all clips with unique IDs\n",
        "for split in [\"train\",\"validation\",\"test\"]:\n",
        "    for emo in EMOTIONS:\n",
        "        vids = glob.glob(f\"{CAER_ROOT}/{split}/{emo}/*.avi\")\n",
        "        print(f\"📂 {split}/{emo}: {len(vids)} clips\")\n",
        "        for vid_path in tqdm(vids, desc=f\"{split}/{emo}\", ncols=80):\n",
        "            stem = Path(vid_path).stem\n",
        "            cid  = f\"{split}_{emo}_{stem}\"  # unique ID per split/emotion\n",
        "\n",
        "            # Text features\n",
        "            try:\n",
        "                txt = torch.tensor(extract_text(vid_path))\n",
        "            except:\n",
        "                txt = ZERO_TEXT\n",
        "            out[\"CMU_MOSEI_TimestampedWordVectors\"][cid] = {\"features\": txt}\n",
        "\n",
        "            # Audio features\n",
        "            try:\n",
        "                aud = extract_audio(vid_path)\n",
        "            except:\n",
        "                aud = ZERO_AUD\n",
        "            out[\"CMU_MOSEI_COVAREP\"][cid] = {\"features\": aud}\n",
        "\n",
        "            # Visual features\n",
        "            try:\n",
        "                vis = extract_visual(vid_path)\n",
        "            except:\n",
        "                vis = ZERO_VIS\n",
        "            out[\"CMU_MOSEI_VisualFacet42\"][cid] = {\"features\": vis}\n",
        "\n",
        "            # One-hot label\n",
        "            oh = np.zeros(len(EMOTIONS), np.float32)\n",
        "            oh[CAER2MOSEI[EMOTIONS.index(emo)]] = 1\n",
        "            out[\"CMU_MOSEI_Labels\"][cid] = {\"features\": oh}\n",
        "\n",
        "# 8. Save the full-run pickle\n",
        "with open(OUT_PKL_PATH, \"wb\") as f:\n",
        "    pickle.dump(out, f)\n",
        "print(f\"✅ Saved full-run pickle with {len(out['CMU_MOSEI_Labels'])} clips → {OUT_PKL_PATH}\")\n"
      ],
      "metadata": {
        "id": "N0-6NL_cIrP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from torchmetrics.classification import MultilabelF1Score, MultilabelAccuracy\n",
        "\n",
        "\n",
        "class CAERDataset(Dataset):\n",
        "    def __init__(self, pkl_path):\n",
        "        with open(pkl_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self.data = data\n",
        "        self.ids  = list(data['CMU_MOSEI_Labels'].keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        k = self.ids[idx]\n",
        "        t = torch.as_tensor(self.data['CMU_MOSEI_TimestampedWordVectors'][k]['features'])\n",
        "        a = torch.as_tensor(self.data['CMU_MOSEI_COVAREP'][k]['features'])\n",
        "        v = torch.as_tensor(self.data['CMU_MOSEI_VisualFacet42'][k]['features'])\n",
        "        y = torch.as_tensor(self.data['CMU_MOSEI_Labels'][k]['features'])\n",
        "        return t, a, v, y\n",
        "\n",
        "def coll(batch):\n",
        "    ts, as_, vs, ys = zip(*batch)\n",
        "    t = torch.stack(ts).unsqueeze(1)       # (B,1,300)\n",
        "    tlen = torch.ones(len(ts), dtype=torch.long)\n",
        "    alens = torch.tensor([a.size(0) for a in as_])\n",
        "    a_padded = pad_sequence(as_, batch_first=True)   # (B,Ta,65)\n",
        "    vlens = torch.tensor([v.size(0) for v in vs])\n",
        "    v_padded = pad_sequence(vs, batch_first=True)    # (B,Tv,2048)\n",
        "    y = torch.stack(ys)                              # (B,7)\n",
        "    return t, tlen, a_padded, alens, v_padded, vlens, y\n",
        "\n",
        "\n",
        "class XModal(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.text_rnn   = nn.GRU(300, 256, batch_first=True)\n",
        "        self.audio_rnn  = nn.GRU(74, 128, batch_first=True)\n",
        "        self.visual_rnn = nn.GRU(35, 128, batch_first=True)\n",
        "        self.fc         = nn.Linear(256 + 128 + 128, 7)\n",
        "\n",
        "        self.loss   = nn.BCEWithLogitsLoss()   # no pos_weight for main run\n",
        "        self.thresh = 0.3\n",
        "        self._micro = MultilabelF1Score(7, threshold=self.thresh, average='micro')\n",
        "        self._acc   = MultilabelAccuracy(7, threshold=self.thresh)\n",
        "\n",
        "    def _last(self, rnn, x, lengths):\n",
        "        packed, _ = pack_padded_sequence(x, lengths.cpu(),\n",
        "                                         batch_first=True,\n",
        "                                         enforce_sorted=False), None\n",
        "        return rnn(packed)[1].squeeze(0)\n",
        "\n",
        "    def forward(self, t, tlen, a, alen, v, vlen):\n",
        "        h = torch.cat([\n",
        "            self._last(self.text_rnn,   t,    tlen),\n",
        "            self._last(self.audio_rnn,  a,    alen),\n",
        "            self._last(self.visual_rnn, v,    vlen),\n",
        "        ], dim=-1)\n",
        "        return self.fc(h)\n",
        "\n",
        "    def training_step(self, batch, _):\n",
        "        t, tlen, a, alen, v, vlen, y = batch\n",
        "        logits = self(t, tlen, a, alen, v, vlen)\n",
        "        loss = self.loss(logits, y)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, _):\n",
        "        t, tlen, a, alen, v, vlen, y = batch\n",
        "        logits = self(t, tlen, a, alen, v, vlen)\n",
        "        preds  = (torch.sigmoid(logits) > self.thresh).int()\n",
        "        self._micro.update(preds, y.int())\n",
        "        self._acc.update(preds, y.int())\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        self.log(\"val_f1_micro\", self._micro.compute(), prog_bar=True)\n",
        "        self.log(\"val_acc\",      self._acc.compute(),    prog_bar=True)\n",
        "        self._micro.reset(); self._acc.reset()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-4, amsgrad=True)\n",
        "\n",
        "\n",
        "class XModalProj(pl.LightningModule):\n",
        "    def __init__(self, ckpt_path):\n",
        "        super().__init__()\n",
        "        # load MOSEI weights into backbone\n",
        "        base = XModal.load_from_checkpoint(ckpt_path, strict=False)\n",
        "        self.text_rnn   = base.text_rnn\n",
        "        self.audio_rnn  = base.audio_rnn\n",
        "        self.visual_rnn = base.visual_rnn\n",
        "        self.fc         = base.fc\n",
        "\n",
        "        # projection layers (CAER→MOSEI dims)\n",
        "        self.a_proj = nn.Linear(65,   74,   bias=False)\n",
        "        self.v_proj = nn.Linear(2048, 35,   bias=True)\n",
        "        nn.init.eye_(self.a_proj.weight)\n",
        "        nn.init.zeros_(self.v_proj.weight)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.loss    = nn.BCEWithLogitsLoss()\n",
        "        self.thresh  = base.thresh\n",
        "        self.f1      = MultilabelF1Score(7, threshold=self.thresh, average='micro')\n",
        "\n",
        "    def _last(self, rnn, x, lengths):\n",
        "        packed = pack_padded_sequence(x, lengths.cpu(),\n",
        "                                      batch_first=True,\n",
        "                                      enforce_sorted=False)\n",
        "        return rnn(packed)[1].squeeze(0)\n",
        "\n",
        "    def forward(self, t, tlen, a, alen, v, vlen):\n",
        "        a_proj = self.a_proj(a)\n",
        "        v_proj = self.v_proj(v)\n",
        "        h = torch.cat([\n",
        "            self._last(self.text_rnn,   t,    tlen),\n",
        "            self._last(self.audio_rnn,  a_proj, alen),\n",
        "            self._last(self.visual_rnn, v_proj, vlen),\n",
        "        ], dim=-1)\n",
        "        return self.fc(self.dropout(h))\n",
        "\n",
        "    def training_step(self, batch, _):\n",
        "        t, tlen, a, alen, v, vlen, y = batch\n",
        "        logits = self(t, tlen, a, alen, v, vlen)\n",
        "        loss   = self.loss(logits, y)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, _):\n",
        "        t, tlen, a, alen, v, vlen, y = batch\n",
        "        logits = self(t, tlen, a, alen, v, vlen)\n",
        "        preds  = (torch.sigmoid(logits) > self.thresh).int()\n",
        "        self.f1.update(preds, y.int())\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        self.log(\"val_f1\", self.f1.compute(), prog_bar=True)\n",
        "        self.f1.reset()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        pretrained = (list(self.text_rnn.parameters()) +\n",
        "                      list(self.audio_rnn.parameters()) +\n",
        "                      list(self.visual_rnn.parameters()))\n",
        "        new_params = (list(self.a_proj.parameters()) +\n",
        "                      list(self.v_proj.parameters()) +\n",
        "                      list(self.fc.parameters()))\n",
        "        return torch.optim.Adam([\n",
        "            {'params': pretrained, 'lr': 1e-5},\n",
        "            {'params': new_params,  'lr': 1e-4}\n",
        "        ], amsgrad=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    DRIVE_ROOT   = \"/content/drive/MyDrive\"\n",
        "    CAER_PKL     = f\"{DRIVE_ROOT}/caer_aligned_full.pkl\"\n",
        "    MOSEI_CKPT   = f\"{DRIVE_ROOT}/EmotionFusion/checkpoints/best-epoch=12-val_f1_micro=0.459.ckpt\"\n",
        "    CKPT_DIR     = f\"{DRIVE_ROOT}/CAER_finetune_checkpoints\"\n",
        "\n",
        "    os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "    ds      = CAERDataset(CAER_PKL)\n",
        "    N       = len(ds)\n",
        "    train_s = int(0.7 * N)\n",
        "    val_s   = int(0.15 * N)\n",
        "    test_s  = N - train_s - val_s\n",
        "    train_ds, val_ds, _ = random_split(ds, [train_s, val_s, test_s])\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  collate_fn=coll, num_workers=4)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, collate_fn=coll, num_workers=4)\n",
        "\n",
        "    ckpt_cb = ModelCheckpoint(\n",
        "        dirpath=CKPT_DIR,\n",
        "        filename=\"best-caer-{epoch:02d}-{val_f1:.3f}\",\n",
        "        monitor=\"val_f1\",\n",
        "        mode=\"max\",\n",
        "        save_top_k=1\n",
        "    )\n",
        "    stop_cb = EarlyStopping(monitor=\"val_f1\", mode=\"max\", patience=5, verbose=True)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        max_epochs=30,\n",
        "        accelerator=\"auto\",\n",
        "        callbacks=[ckpt_cb, stop_cb]\n",
        "    )\n",
        "\n",
        "    model_ft = XModalProj(MOSEI_CKPT)\n",
        "    print(\"⚙️ Starting fine-tuning on CAER...\")\n",
        "    trainer.fit(model_ft, train_loader, val_loader)\n",
        "    print(\"✅ Done. Best checkpoint →\", ckpt_cb.best_model_path)\n"
      ],
      "metadata": {
        "id": "yxFZRZcATUwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics.classification import (\n",
        "    MultilabelF1Score,\n",
        "    MultilabelPrecision,\n",
        "    MultilabelRecall,\n",
        "    MultilabelAccuracy\n",
        ")\n",
        "\n",
        "# ---------------------- CAER Dataset ----------------------\n",
        "class CAERDataset(Dataset):\n",
        "    def __init__(self, pkl_path):\n",
        "        with open(pkl_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self.data = data\n",
        "        self.ids  = list(data['CMU_MOSEI_Labels'].keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        k = self.ids[idx]\n",
        "        t = torch.as_tensor(self.data['CMU_MOSEI_TimestampedWordVectors'][k]['features'])\n",
        "        a = torch.as_tensor(self.data['CMU_MOSEI_COVAREP'][k]['features'])\n",
        "        v = torch.as_tensor(self.data['CMU_MOSEI_VisualFacet42'][k]['features'])\n",
        "        y = torch.as_tensor(self.data['CMU_MOSEI_Labels'][k]['features'])\n",
        "        return t, a, v, y\n",
        "\n",
        "def coll(batch):\n",
        "    ts, as_, vs, ys = zip(*batch)\n",
        "    t = torch.stack(ts).unsqueeze(1)                 # (B,1,300)\n",
        "    tlen = torch.ones(len(ts), dtype=torch.long)     # all length=1\n",
        "\n",
        "    alens = torch.tensor([a.shape[0] for a in as_])\n",
        "    a_padded = pad_sequence(as_, batch_first=True)   # (B, T_a, 65)\n",
        "\n",
        "    vlens = torch.tensor([v.shape[0] for v in vs])\n",
        "    v_padded = pad_sequence(vs, batch_first=True)    # (B, T_v, 2048)\n",
        "\n",
        "    y = torch.stack(ys)                              # (B,7)\n",
        "    return t, tlen, a_padded, alens, v_padded, vlens, y\n",
        "\n",
        "# ---------------------- Fine-tunable Model Definition ----------------------\n",
        "class XModalProj(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define MOSEI‐pretrained architecture slots to be filled by checkpoint\n",
        "        self.text_rnn   = nn.GRU(300, 256, batch_first=True)\n",
        "        self.audio_rnn  = nn.GRU(74,  128, batch_first=True)\n",
        "        self.visual_rnn = nn.GRU(35,  128, batch_first=True)\n",
        "        self.fc         = nn.Linear(256 + 128 + 128, 7)\n",
        "\n",
        "        # Projection layers for CAER→MOSEI dims\n",
        "        self.a_proj = nn.Linear(65,   74, bias=False)\n",
        "        self.v_proj = nn.Linear(2048, 35, bias=True)\n",
        "        nn.init.eye_( self.a_proj.weight )\n",
        "        nn.init.zeros_(self.v_proj.weight)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.loss    = nn.BCEWithLogitsLoss()\n",
        "        self.thresh  = 0.3\n",
        "\n",
        "    def forward(self, t, tlen, a, alen, v, vlen):\n",
        "        # project CAER features into MOSEI dims\n",
        "        a_proj = self.a_proj(a)\n",
        "        v_proj = self.v_proj(v)\n",
        "\n",
        "        # helper to get last hidden state\n",
        "        def last(rnn, x, lengths):\n",
        "            packed = pack_padded_sequence(x, lengths.cpu(),\n",
        "                                          batch_first=True,\n",
        "                                          enforce_sorted=False)\n",
        "            return rnn(packed)[1].squeeze(0)\n",
        "\n",
        "        h_text  = last(self.text_rnn,   t,      tlen)\n",
        "        h_audio = last(self.audio_rnn,  a_proj, alen)\n",
        "        h_vis   = last(self.visual_rnn, v_proj, vlen)\n",
        "        h       = torch.cat([h_text, h_audio, h_vis], dim=-1)\n",
        "        h       = self.dropout(h)\n",
        "        return self.fc(h)\n",
        "\n",
        "# ---------------------- Evaluation Script ----------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Paths\n",
        "    DRIVE_ROOT     = \"/content/drive/MyDrive\"\n",
        "    PICKLE_PATH    = f\"{DRIVE_ROOT}/caer_aligned_full.pkl\"\n",
        "    BEST_CKPT_PATH = f\"{DRIVE_ROOT}/CAER_finetune_checkpoints/best-caer-epoch=25-val_f1=0.391.ckpt\"\n",
        "\n",
        "    # Load dataset and split\n",
        "    ds = CAERDataset(PICKLE_PATH)\n",
        "    N  = len(ds)\n",
        "    train_sz = int(0.7 * N)\n",
        "    val_sz   = int(0.15 * N)\n",
        "    test_sz  = N - train_sz - val_sz\n",
        "    _, _, test_ds = random_split(ds, [train_sz, val_sz, test_sz])\n",
        "    test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=coll)\n",
        "\n",
        "    # Load model from checkpoint (fills text_rnn, audio_rnn, visual_rnn, fc)\n",
        "    model = XModalProj.load_from_checkpoint(BEST_CKPT_PATH, strict=False)\n",
        "    model.eval().cuda()\n",
        "\n",
        "    # Metrics\n",
        "    threshold = 0.3\n",
        "    f1    = MultilabelF1Score(num_labels=7, threshold=threshold, average='none').cuda()\n",
        "    prec  = MultilabelPrecision(num_labels=7, threshold=threshold, average='none').cuda()\n",
        "    rec   = MultilabelRecall(num_labels=7, threshold=threshold, average='none').cuda()\n",
        "    acc   = MultilabelAccuracy(num_labels=7, threshold=threshold, average='none').cuda()\n",
        "\n",
        "    # Run evaluation\n",
        "    print(\"🔍 Evaluating on CAER test set...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            t, tlen, a, alen, v, vlen, y = [x.cuda() for x in batch]\n",
        "            logits = model(t, tlen, a, alen, v, vlen)\n",
        "            preds  = (torch.sigmoid(logits) > threshold).int()\n",
        "            y_bin  = y.int().cuda()\n",
        "            f1.update(preds, y_bin)\n",
        "            prec.update(preds, y_bin)\n",
        "            rec.update(preds, y_bin)\n",
        "            acc.update(preds, y_bin)\n",
        "\n",
        "    # Report results\n",
        "    class_names = [\"happy\",\"sad\",\"angry\",\"surprise\",\"disgust\",\"fear\",\"neutral\"]\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame({\n",
        "        \"emotion\":   class_names,\n",
        "        \"F1\":        f1.compute().cpu().numpy(),\n",
        "        \"Precision\": prec.compute().cpu().numpy(),\n",
        "        \"Recall\":    rec.compute().cpu().numpy(),\n",
        "        \"Accuracy\":  acc.compute().cpu().numpy()\n",
        "    })\n",
        "    print(df.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "mwlmJNAMF0Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from torchmetrics.classification import MultilabelF1Score, MultilabelAccuracy\n",
        "\n",
        "# 1. Load your 28‐movie pickle\n",
        "with open(\"/content/drive/MyDrive/movie_clips_simple.pkl\", \"rb\") as f:\n",
        "    movie_data = pickle.load(f)\n",
        "clip_ids = list(movie_data[\"CMU_MOSEI_Labels\"].keys())\n",
        "print(f\"🔢 Loaded {len(clip_ids)} movie clips\")\n",
        "\n",
        "# 2. Dataset & collate\n",
        "class MovieDS(Dataset):\n",
        "    def __init__(self, ids): self.ids = ids\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        k = self.ids[idx]; d = movie_data\n",
        "        t = torch.tensor(d[\"CMU_MOSEI_TimestampedWordVectors\"][k][\"features\"], dtype=torch.float32)\n",
        "        a = torch.tensor(d[\"CMU_MOSEI_COVAREP\"][k][\"features\"], dtype=torch.float32)\n",
        "        v = torch.tensor(d[\"CMU_MOSEI_VisualFacet42\"][k][\"features\"], dtype=torch.float32)\n",
        "        y = torch.tensor(d[\"CMU_MOSEI_Labels\"][k][\"features\"], dtype=torch.float32)\n",
        "        return t, a, v, y\n",
        "\n",
        "def collate(batch):\n",
        "    t,a,v,y = zip(*batch)\n",
        "    tlen = torch.tensor([x.size(0) for x in t])\n",
        "    alen = torch.tensor([x.size(0) for x in a])\n",
        "    vlen = torch.tensor([x.size(0) for x in v])\n",
        "    return (\n",
        "        pad_sequence(t, True), tlen,\n",
        "        pad_sequence(a, True), alen,\n",
        "        pad_sequence(v, True), vlen,\n",
        "        torch.stack(y)\n",
        "    )\n",
        "\n",
        "loader = DataLoader(MovieDS(clip_ids), batch_size=16, shuffle=False, collate_fn=collate)\n",
        "\n",
        "# 3. Model definition (must match training)\n",
        "class XModal(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.text_rnn   = nn.GRU(300,256,batch_first=True)\n",
        "        self.audio_rnn  = nn.GRU( 74,128,batch_first=True)\n",
        "        self.visual_rnn = nn.GRU( 35,128,batch_first=True)\n",
        "        self.fc         = nn.Linear(256+128+128, 7)\n",
        "        self.thresh     = 0.30\n",
        "\n",
        "    def _last(self, rnn, x, lengths):\n",
        "        _, h_n = rnn(pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False))\n",
        "        return h_n.squeeze(0)\n",
        "\n",
        "    def forward(self, t, tlen, a, alen, v, vlen):\n",
        "        h = torch.cat([\n",
        "            self._last(self.text_rnn,   t, tlen),\n",
        "            self._last(self.audio_rnn,  a, alen),\n",
        "            self._last(self.visual_rnn, v, vlen),\n",
        "        ], dim=-1)\n",
        "        return self.fc(h)\n",
        "\n",
        "# 4. Evaluation function\n",
        "def eval_model(ckpt_path, loader, device=\"cuda\"):\n",
        "    # load checkpoint\n",
        "    model = XModal.load_from_checkpoint(ckpt_path, strict=False).eval().to(device)\n",
        "    thresh = model.thresh\n",
        "    f1_metric = MultilabelF1Score(num_labels=7, threshold=thresh, average=\"macro\").to(device)\n",
        "    acc_metric = MultilabelAccuracy(num_labels=7, threshold=thresh).to(device)\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = [x.to(device) for x in batch]\n",
        "            logits = model(*batch[:-1])\n",
        "            preds  = (torch.sigmoid(logits) > thresh).int()\n",
        "            target = (batch[-1] > 0).int()\n",
        "            f1_metric.update(preds, target)\n",
        "            acc_metric.update(preds, target)\n",
        "    return f1_metric.compute().item(), acc_metric.compute().item()\n",
        "\n",
        "# 5. Paths to your two checkpoints\n",
        "zero_ckpt = \"/content/drive/MyDrive/EmotionFusion/checkpoints/best-epoch=12-val_f1_micro=0.459.ckpt\"\n",
        "caer_ckpt = \"/content/drive/MyDrive/CAER_finetune_checkpoints/best-caer-epoch=25-val_f1=0.391.ckpt\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"🔍 Evaluating zero‐shot MOSEI model...\")\n",
        "f1_zero, acc_zero = eval_model(zero_ckpt, loader, device)\n",
        "print(f\"Zero-shot MOSEI → Movie clips  | Macro-F1: {f1_zero:.4f} | Accuracy: {acc_zero:.4f}\")\n",
        "\n",
        "print(\"\\n🔍 Evaluating CAER-fine-tuned model...\")\n",
        "f1_caer, acc_caer = eval_model(caer_ckpt, loader, device)\n",
        "print(f\"CAER-fine-tuned → Movie clips | Macro-F1: {f1_caer:.4f} | Accuracy: {acc_caer:.4f}\")\n"
      ],
      "metadata": {
        "id": "kLAk2T9BVF1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Install & enable Git LFS\n",
        "!apt-get update -qq && apt-get install -qq git-lfs\n",
        "!git lfs install\n",
        "\n",
        "# 2) Clone the CREMA-D repo (with all FLV video files)\n",
        "!git clone https://github.com/CheyneyComputerScience/CREMA-D.git /content/CREMA-D\n",
        "\n",
        "# 3) Verify you have the FLVs\n",
        "!ls /content/CREMA-D/VideoFlash | head -n 10\n"
      ],
      "metadata": {
        "id": "gshMdP24d_WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, subprocess\n",
        "\n",
        "\n",
        "out_dir = \"/content/drive/MyDrive/CREMA-D_MP4\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "flvs = glob.glob(\"/content/CREMA-D/VideoFlash/*.flv\")\n",
        "\n",
        "\n",
        "for f in flvs:\n",
        "    name = os.path.basename(f).rsplit(\".\", 1)[0]\n",
        "    mp4 = os.path.join(out_dir, f\"{name}.mp4\")\n",
        "    print(\"Converting\", f, \"→\", mp4)\n",
        "    subprocess.run([\n",
        "        \"ffmpeg\", \"-y\", \"-i\", f,\n",
        "        \"-c:v\", \"libx264\", \"-preset\", \"fast\", \"-crf\", \"23\",\n",
        "        \"-c:a\", \"aac\", \"-b:a\", \"128k\",\n",
        "        mp4\n",
        "    ], check=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "6tyuyLU3jC68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from pathlib import Path\n",
        "import pickle, numpy as np, cv2, subprocess, tempfile\n",
        "import torch, torchaudio, torchvision\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.decomposition import PCA\n",
        "import torchvision.transforms as T\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "video_root = Path(\"/content/drive/MyDrive/CREMA-D_MP4\")\n",
        "assert video_root.exists(), \"Make sure CREMA-D_MP4 exists!\"\n",
        "\n",
        "mel74 = torchaudio.transforms.MelSpectrogram(sample_rate=16_000, n_mels=74)\n",
        "resnet_full = torchvision.models.resnet18(weights=\"DEFAULT\").eval()\n",
        "feature_net = torch.nn.Sequential(*list(resnet_full.children())[:-1]).eval()\n",
        "to224 = T.Compose([T.ToPILImage(), T.Resize(224), T.ToTensor()])\n",
        "\n",
        "\n",
        "pca35 = PCA(35).fit(np.random.randn(400, 512))\n",
        "\n",
        "\n",
        "def extract_covarep_from_mp4(mp4_path):\n",
        "    # Extract audio track to temp wav, then compute mel74→(1,74)\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".wav\") as tmp:\n",
        "        subprocess.run([\n",
        "            \"ffmpeg\", \"-y\", \"-i\", str(mp4_path),\n",
        "            \"-ar\", \"16000\", \"-ac\", \"1\", \"-loglevel\", \"error\",\n",
        "            tmp.name\n",
        "        ], check=True)\n",
        "        sig, _ = torchaudio.load(tmp.name)\n",
        "    return mel74(sig)[0].mean(-1, keepdim=True).T.numpy().astype(np.float32)\n",
        "\n",
        "def extract_visualfacet42_from_mp4(mp4_path):\n",
        "    cap = cv2.VideoCapture(str(mp4_path))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 25\n",
        "    step = max(int(fps), 1)\n",
        "    frames, f = [], 0\n",
        "    with torch.no_grad():\n",
        "        while True:\n",
        "            ok, frame = cap.read()\n",
        "            if not ok:\n",
        "                break\n",
        "            if f % step == 0:\n",
        "                x   = to224(frame[:, :, ::-1]).unsqueeze(0)     # (1,3,224,224)\n",
        "                vec = feature_net(x).squeeze().numpy()          # (512,)\n",
        "                frames.append(pca35.transform(vec[None])[0])    # (35,)\n",
        "            f += 1\n",
        "    cap.release()\n",
        "    return (np.stack(frames, axis=0) if frames else np.zeros((1,35),np.float32)).astype(np.float32)\n",
        "\n",
        "\n",
        "out = {\n",
        "    \"CMU_MOSEI_TimestampedWordVectors\": {},\n",
        "    \"CMU_MOSEI_COVAREP\": {},\n",
        "    \"CMU_MOSEI_VisualFacet42\": {},\n",
        "    \"CMU_MOSEI_Labels\": {}\n",
        "}\n",
        "\n",
        "\n",
        "code2idx = {\"ANG\":0,\"DIS\":1,\"HAP\":2,\"SAD\":3,\"FEA\":4,\"NEU\":5}\n",
        "\n",
        "\n",
        "for idx, mp4_fp in enumerate(tqdm(sorted(video_root.glob(\"*.mp4\")))):\n",
        "    base = mp4_fp.stem   # e.g. \"1001_DFA_ANG_XX\"\n",
        "    key  = f\"{idx:05d}_{base}\"\n",
        "\n",
        "\n",
        "    out[\"CMU_MOSEI_TimestampedWordVectors\"][key] = {\n",
        "        \"features\": np.zeros((1,300), np.float32)\n",
        "    }\n",
        "\n",
        "\n",
        "    cov = extract_covarep_from_mp4(mp4_fp)\n",
        "    out[\"CMU_MOSEI_COVAREP\"][key] = {\"features\": cov}\n",
        "\n",
        "\n",
        "    vis = extract_visualfacet42_from_mp4(mp4_fp)\n",
        "    out[\"CMU_MOSEI_VisualFacet42\"][key] = {\"features\": vis}\n",
        "\n",
        "    parts = base.split(\"_\")\n",
        "    emo_code = parts[2] if len(parts) > 2 else None\n",
        "    y7 = np.zeros(7, np.float32)\n",
        "    if emo_code in code2idx:\n",
        "        y7[code2idx[emo_code]] = 1.0\n",
        "    out[\"CMU_MOSEI_Labels\"][key] = {\"features\": y7}\n",
        "\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/crema_d_features.pkl\"\n",
        "with open(save_path, \"wb\") as f:\n",
        "    pickle.dump(out, f)\n",
        "\n",
        "print(f\"🎉 Saved {len(out['CMU_MOSEI_Labels'])} CREMA-D feature entries → {save_path}\")\n"
      ],
      "metadata": {
        "id": "n3MVfNqDq5HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from torch import nn\n",
        "from torchmetrics.classification import MultilabelF1Score, MultilabelAccuracy\n",
        "\n",
        "# 1️⃣ Load CREMA-D features pickle\n",
        "CREMA_PKL = \"/content/drive/MyDrive/crema_d_features.pkl\"\n",
        "with open(CREMA_PKL, \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "uids = list(data[\"CMU_MOSEI_Labels\"].keys())\n",
        "\n",
        "# 2️⃣ Dataset & collate\n",
        "class CremaDataset(Dataset):\n",
        "    def __init__(self, uids, data):\n",
        "        self.uids = uids\n",
        "        self.data = data\n",
        "    def __len__(self):\n",
        "        return len(self.uids)\n",
        "    def __getitem__(self, idx):\n",
        "        uid = self.uids[idx]\n",
        "        t = torch.from_numpy(self.data[\"CMU_MOSEI_TimestampedWordVectors\"][uid][\"features\"])\n",
        "        a = torch.from_numpy(self.data[\"CMU_MOSEI_COVAREP\"][uid][\"features\"])\n",
        "        v = torch.from_numpy(self.data[\"CMU_MOSEI_VisualFacet42\"][uid][\"features\"])\n",
        "        y = torch.from_numpy(self.data[\"CMU_MOSEI_Labels\"][uid][\"features\"])\n",
        "        return t, a, v, y\n",
        "\n",
        "def coll(batch):\n",
        "    ts, as_, vs, ys = zip(*batch)\n",
        "    # pad text (B, T_t, 300)\n",
        "    t_p = pad_sequence(ts, batch_first=True)\n",
        "    tlen = torch.tensor([t.size(0) for t in ts])\n",
        "    # pad audio (B, T_a, 65)\n",
        "    a_p = pad_sequence(as_, batch_first=True)\n",
        "    alen = torch.tensor([a.size(0) for a in as_])\n",
        "    # pad visual (B, T_v, 2048)\n",
        "    v_p = pad_sequence(vs, batch_first=True)\n",
        "    vlen = torch.tensor([v.size(0) for v in vs])\n",
        "    # labels (B,7)\n",
        "    y = torch.stack(ys)\n",
        "    return t_p, tlen, a_p, alen, v_p, vlen, y\n",
        "\n",
        "dataset = CremaDataset(uids, data)\n",
        "loader  = DataLoader(dataset, batch_size=16, collate_fn=coll, num_workers=2)\n",
        "\n",
        "# 3️⃣ Define the core model class\n",
        "class XModal(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.text_rnn   = nn.GRU(300,256,batch_first=True)\n",
        "        self.audio_rnn  = nn.GRU(74,128,batch_first=True)\n",
        "        self.visual_rnn = nn.GRU(35,128,batch_first=True)\n",
        "        self.fc         = nn.Linear(256+128+128, 7)\n",
        "        self.thresh     = 0.3\n",
        "\n",
        "    def forward(self, t, tlen, a, alen, v, vlen):\n",
        "        def last(rnn, x, l):\n",
        "            return rnn(pack_padded_sequence(x, l.cpu(), batch_first=True, enforce_sorted=False))[1].squeeze(0)\n",
        "        h = torch.cat([\n",
        "            last(self.text_rnn,   t,    tlen),\n",
        "            last(self.audio_rnn,  a,    alen),\n",
        "            last(self.visual_rnn, v,    vlen),\n",
        "        ], dim=-1)\n",
        "        return self.fc(h)\n",
        "\n",
        "# 4️⃣ Evaluation function\n",
        "def evaluate_model(ckpt_path, loader, device=\"cuda\"):\n",
        "    # load model\n",
        "    model = XModal().to(device)\n",
        "    state = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(state[\"state_dict\"], strict=False)\n",
        "    model.eval()\n",
        "\n",
        "    f1_metric = MultilabelF1Score(num_labels=7, threshold=model.thresh, average=\"macro\").to(device)\n",
        "    acc_metric = MultilabelAccuracy    (num_labels=7, threshold=model.thresh).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            t, tlen, a, alen, v, vlen, y = [b.to(device) for b in batch]\n",
        "            logits = model(t, tlen, a, alen, v, vlen)\n",
        "            preds  = (torch.sigmoid(logits) > model.thresh).int()\n",
        "            target = y.int()\n",
        "            f1_metric.update(preds, target)\n",
        "            acc_metric.update(preds, target)\n",
        "\n",
        "    f1  = f1_metric.compute().item()\n",
        "    acc = acc_metric.compute().item()\n",
        "    return f1, acc\n",
        "\n",
        "# 5️⃣ Paths to your checkpoints\n",
        "MOSEI_CKPT = \"/content/drive/MyDrive/EmotionFusion/checkpoints/best-epoch=12-val_f1_micro=0.459.ckpt\"\n",
        "CAER_CKPT  = \"/content/drive/MyDrive/CAER_finetune_checkpoints/best-caer-epoch=25-val_f1=0.391.ckpt\"\n",
        "\n",
        "# 6️⃣ Run evaluations\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "f1_zero, acc_zero = evaluate_model(MOSEI_CKPT, loader, device)\n",
        "print(f\"Zero-shot MOSEI → CREMA-D | Macro-F1: {f1_zero:.4f} | Accuracy: {acc_zero:.4f}\")\n",
        "\n",
        "f1_caer, acc_caer = evaluate_model(CAER_CKPT, loader, device)\n",
        "print(f\"CAER-fine-tuned → CREMA-D | Macro-F1: {f1_caer:.4f} | Accuracy: {acc_caer:.4f}\")\n"
      ],
      "metadata": {
        "id": "peeYJCWdtzpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle, torch, numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Load movie-clips pickle\n",
        "with open(\"/content/drive/MyDrive/movie_clips_simple.pkl\",\"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "uids = list(data[\"CMU_MOSEI_Labels\"].keys())\n",
        "\n",
        "# 2) Dataset + collate\n",
        "class MovieDS(Dataset):\n",
        "    def __init__(self,u): self.u=u\n",
        "    def __len__(self): return len(self.u)\n",
        "    def __getitem__(self,i):\n",
        "        uid = self.u[i]\n",
        "        t = torch.from_numpy(data[\"CMU_MOSEI_TimestampedWordVectors\"][uid][\"features\"]).float()\n",
        "        a = torch.from_numpy(data[\"CMU_MOSEI_COVAREP\"][uid][\"features\"]).float()\n",
        "        v = torch.from_numpy(data[\"CMU_MOSEI_VisualFacet42\"][uid][\"features\"]).float()\n",
        "        y = torch.from_numpy(data[\"CMU_MOSEI_Labels\"][uid][\"features\"]).float()\n",
        "        return t, a, v, y\n",
        "\n",
        "def coll(b):\n",
        "    ts,as_,vs,ys = zip(*b)\n",
        "    t = pad_sequence(ts, True); tlen = torch.tensor([x.size(0) for x in ts])\n",
        "    a = pad_sequence(as_,True); alen = torch.tensor([x.size(0) for x in as_])\n",
        "    v = pad_sequence(vs,True); vlen = torch.tensor([x.size(0) for x in vs])\n",
        "    return t,tlen,a,alen,v,vlen,torch.stack(ys)\n",
        "\n",
        "loader = DataLoader(MovieDS(uids), batch_size=16, collate_fn=coll)\n",
        "\n",
        "# 3) Core model\n",
        "class XModal(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.text_rnn   = torch.nn.GRU(300,256,batch_first=True)\n",
        "        self.audio_rnn  = torch.nn.GRU(74,128,batch_first=True)\n",
        "        self.visual_rnn = torch.nn.GRU(35,128,batch_first=True)\n",
        "        self.fc         = torch.nn.Linear(256+128+128,7)\n",
        "    def forward(self,t,tlen,a,alen,v,vlen):\n",
        "        def last(rnn,x,l):\n",
        "            return rnn(pack_padded_sequence(x,l.cpu(),batch_first=True,enforce_sorted=False))[1].squeeze(0)\n",
        "        h = torch.cat([\n",
        "            last(self.text_rnn,   t,    tlen),\n",
        "            last(self.audio_rnn,  a,    alen),\n",
        "            last(self.visual_rnn, v,    vlen)\n",
        "        ], dim=-1)\n",
        "        return self.fc(h)\n",
        "\n",
        "# 4) Checkpoints\n",
        "paths = {\n",
        "  \"Zero-shot MOSEI\": \"/content/drive/MyDrive/EmotionFusion/checkpoints/best-epoch=12-val_f1_micro=0.459.ckpt\",\n",
        "  \"CAER-fine-tuned\": \"/content/drive/MyDrive/CAER_finetune_checkpoints/best-caer-epoch=25-val_f1=0.391.ckpt\"\n",
        "}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "results = {}\n",
        "\n",
        "# Prepare plots\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "for i, (name, ckpt) in enumerate(paths.items(), 1):\n",
        "    # load model\n",
        "    model = XModal().to(device)\n",
        "    sd = torch.load(ckpt, map_location=device)[\"state_dict\"]\n",
        "    model.load_state_dict(sd, strict=False)\n",
        "    model.eval()\n",
        "\n",
        "    # collect logits & targets\n",
        "    all_logits, all_targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for t,tlen,a,alen,v,vlen,y in loader:\n",
        "            t, a, v = t.to(device), a.to(device), v.to(device)\n",
        "            logits = model(t,tlen,a,alen,v,vlen).cpu().numpy()\n",
        "            all_logits.append(logits)\n",
        "            all_targets.append(y.numpy())\n",
        "    logits = np.concatenate(all_logits)\n",
        "    targets = np.concatenate(all_targets)\n",
        "\n",
        "    # sweep thresholds\n",
        "    ths = np.linspace(0.1, 0.9, 17)\n",
        "    f1s, accs = [], []\n",
        "    for th in ths:\n",
        "        preds = (1/(1+np.exp(-logits)) > th).astype(int)\n",
        "        f1s.append(f1_score(targets, preds, average=\"macro\", zero_division=0))\n",
        "        accs.append((preds == targets).mean())\n",
        "\n",
        "    # find best F1 and best accuracy thresholds\n",
        "    best_f1_idx  = int(np.argmax(f1s))\n",
        "    best_acc_idx = int(np.argmax(accs))\n",
        "    results[name] = {\n",
        "        \"best_f1\":  (ths[best_f1_idx], f1s[best_f1_idx]),\n",
        "        \"best_acc\": (ths[best_acc_idx], accs[best_acc_idx])\n",
        "    }\n",
        "\n",
        "    # plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(ths, f1s, marker='o', label=name)\n",
        "    plt.title(\"Macro-F1 vs Threshold\")\n",
        "    plt.xlabel(\"Threshold\"); plt.ylabel(\"Macro-F1\"); plt.grid(True)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(ths, accs, marker='o', label=name)\n",
        "    plt.title(\"Accuracy vs Threshold\")\n",
        "    plt.xlabel(\"Threshold\"); plt.ylabel(\"Accuracy\"); plt.grid(True)\n",
        "\n",
        "# finalize\n",
        "for ax in plt.gcf().axes:\n",
        "    ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# print summary\n",
        "for name, stats in results.items():\n",
        "    th_f1,  f1  = stats[\"best_f1\"]\n",
        "    th_acc, acc = stats[\"best_acc\"]\n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  Best Macro-F1  = {f1:.4f} at threshold {th_f1:.2f}\")\n",
        "    print(f\"  Best Accuracy  = {acc:.4f} at threshold {th_acc:.2f}\")\n"
      ],
      "metadata": {
        "id": "q5F8O3HX_4Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle, torch, numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# --- helper to evaluate one dataset + model ckpt ---\n",
        "def evaluate_at_best_f1(data_pkl, ckpt_path, batch_size=16, device=\"cpu\"):\n",
        "    # 1) load features\n",
        "    with open(data_pkl, \"rb\") as f:\n",
        "        data = pickle.load(f)\n",
        "    uids = list(data[\"CMU_MOSEI_Labels\"].keys())\n",
        "\n",
        "    # 2) dataset + collate\n",
        "    class DS(Dataset):\n",
        "        def __init__(self,u): self.u=u\n",
        "        def __len__(self): return len(self.u)\n",
        "        def __getitem__(self,i):\n",
        "            uid = self.u[i]\n",
        "            t = torch.from_numpy(data[\"CMU_MOSEI_TimestampedWordVectors\"][uid][\"features\"]).float()\n",
        "            a = torch.from_numpy(data[\"CMU_MOSEI_COVAREP\"][uid][\"features\"]).float()\n",
        "            v = torch.from_numpy(data[\"CMU_MOSEI_VisualFacet42\"][uid][\"features\"]).float()\n",
        "            y = torch.from_numpy(data[\"CMU_MOSEI_Labels\"][uid][\"features\"]).float()\n",
        "            return t,a,v,y\n",
        "\n",
        "    def coll(b):\n",
        "        ts,as_,vs,ys = zip(*b)\n",
        "        t_p = pad_sequence(ts , batch_first=True);   tlen = torch.tensor([x.size(0) for x in ts])\n",
        "        a_p = pad_sequence(as_, batch_first=True);   alen = torch.tensor([x.size(0) for x in as_])\n",
        "        v_p = pad_sequence(vs , batch_first=True);   vlen = torch.tensor([x.size(0) for x in vs])\n",
        "        return t_p,tlen,a_p,alen,v_p,vlen,torch.stack(ys)\n",
        "\n",
        "    loader = DataLoader(DS(uids), batch_size=batch_size, collate_fn=coll)\n",
        "\n",
        "    # 3) model definition must match your XModal\n",
        "    class XModal(torch.nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.text_rnn   = torch.nn.GRU(300,256,batch_first=True)\n",
        "            self.audio_rnn  = torch.nn.GRU( 74,128,batch_first=True)\n",
        "            self.visual_rnn = torch.nn.GRU( 35,128,batch_first=True)\n",
        "            self.fc         = torch.nn.Linear(256+128+128,7)\n",
        "        def forward(self,t,tlen,a,alen,v,vlen):\n",
        "            def last(rnn,x,l):\n",
        "                return rnn(pack_padded_sequence(x,l.cpu(),batch_first=True,enforce_sorted=False))[1].squeeze(0)\n",
        "            h = torch.cat([\n",
        "                last(self.text_rnn,  t,   tlen),\n",
        "                last(self.audio_rnn, a,   alen),\n",
        "                last(self.visual_rnn,v,   vlen),\n",
        "            ], dim=-1)\n",
        "            return self.fc(h)\n",
        "\n",
        "    # 4) load model\n",
        "    model = XModal().to(device)\n",
        "    sd = torch.load(ckpt_path, map_location=device)[\"state_dict\"]\n",
        "    model.load_state_dict(sd, strict=False)\n",
        "    model.eval()\n",
        "\n",
        "    # 5) collect all logits & targets\n",
        "    all_logits, all_targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for t,tlen,a,alen,v,vlen,y in loader:\n",
        "            t,a,v = t.to(device), a.to(device), v.to(device)\n",
        "            logits = model(t,tlen,a,alen,v,vlen).cpu().numpy()\n",
        "            all_logits.append(logits)\n",
        "            all_targets.append(y.numpy())\n",
        "    logits = np.concatenate(all_logits)\n",
        "    targets = np.concatenate(all_targets)\n",
        "\n",
        "    # 6) sweep thresholds\n",
        "    ths = np.linspace(0.1,0.9,17)\n",
        "    best = {\"th\": None, \"f1\": -1, \"acc\": None}\n",
        "    for th in ths:\n",
        "        preds = (1/(1+np.exp(-logits)) > th).astype(int)\n",
        "        f1  = f1_score(targets, preds, average=\"macro\", zero_division=0)\n",
        "        acc = accuracy_score(targets.flatten(), preds.flatten())\n",
        "        if f1 > best[\"f1\"]:\n",
        "            best.update(th=th, f1=f1, acc=acc)\n",
        "    return best\n",
        "\n",
        "# --- run for both datasets & both ckpts ---\n",
        "results = {}\n",
        "models = {\n",
        "    \"Zero-shot MOSEI\": \"/content/drive/MyDrive/EmotionFusion/checkpoints/best-epoch=12-val_f1_micro=0.459.ckpt\",\n",
        "    \"CAER-fine-tuned\": \"/content/drive/MyDrive/CAER_finetune_checkpoints/best-caer-epoch=25-val_f1=0.391.ckpt\"\n",
        "}\n",
        "datasets = {\n",
        "    \"Movie Clips\":   \"/content/drive/MyDrive/movie_clips_simple.pkl\",\n",
        "    \"CREMA-D (4.5k)\":\"/content/drive/MyDrive/crema_d_features.pkl\"\n",
        "}\n",
        "\n",
        "for mdl_name, ckpt in models.items():\n",
        "    for ds_name, pkl in datasets.items():\n",
        "        key = f\"{mdl_name} on {ds_name}\"\n",
        "        results[key] = evaluate_at_best_f1(pkl, ckpt, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- print a clean table ---\n",
        "print(f\"{'Model':<20} {'Dataset':<18} {'Thresh':>6} {'Macro-F1':>10} {'Accuracy':>10}\")\n",
        "print(\"-\"*60)\n",
        "for k,v in results.items():\n",
        "    mdl, ds = k.split(\" on \")\n",
        "    print(f\"{mdl:<20} {ds:<18} {v['th']:6.2f} {v['f1']:10.4f} {v['acc']:10.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kDBanKHuH76g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Load original CAER pickle\n",
        "with open(\"/content/drive/MyDrive/caer_aligned_full.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Prepare new dictionary\n",
        "new_data = {\n",
        "    \"CMU_MOSEI_TimestampedWordVectors\": {},\n",
        "    \"CMU_MOSEI_COVAREP\": {},\n",
        "    \"CMU_MOSEI_VisualFacet42\": {},\n",
        "    \"CMU_MOSEI_Labels\": {}\n",
        "}\n",
        "\n",
        "for k in data['CMU_MOSEI_Labels'].keys():\n",
        "    # TEXT → target shape: [T, 300]\n",
        "    t = data['CMU_MOSEI_TimestampedWordVectors'][k]['features']\n",
        "    if t.ndim == 1:\n",
        "        t = np.expand_dims(t, 0)\n",
        "    t = t[:, :300] if t.shape[1] >= 300 else np.pad(t, ((0,0),(0,300 - t.shape[1])))\n",
        "\n",
        "    # AUDIO → target shape: [T, 74]\n",
        "    a = data['CMU_MOSEI_COVAREP'][k]['features']\n",
        "    a = a[:, :74] if a.shape[1] >= 74 else np.pad(a, ((0,0),(0,74 - a.shape[1])))\n",
        "\n",
        "    # VISUAL → target shape: [T, 35]\n",
        "    v = data['CMU_MOSEI_VisualFacet42'][k]['features']\n",
        "    v = v[:, :35] if v.shape[1] >= 35 else np.pad(v, ((0,0),(0,35 - v.shape[1])))\n",
        "\n",
        "    # LABEL\n",
        "    y = data['CMU_MOSEI_Labels'][k]['features']\n",
        "\n",
        "    # Store in new dict\n",
        "    new_data['CMU_MOSEI_TimestampedWordVectors'][k] = {'features': t}\n",
        "    new_data['CMU_MOSEI_COVAREP'][k] = {'features': a}\n",
        "    new_data['CMU_MOSEI_VisualFacet42'][k] = {'features': v}\n",
        "    new_data['CMU_MOSEI_Labels'][k] = {'features': y}\n",
        "\n",
        "# Save new pickle\n",
        "with open(\"/content/drive/MyDrive/caer_aligned_mosei_format.pkl\", \"wb\") as f:\n",
        "    pickle.dump(new_data, f)\n",
        "\n",
        "print(\"✅ New pickle saved as caer_aligned_mosei_format.pkl\")\n"
      ],
      "metadata": {
        "id": "cEOyG1r_Rdms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle, torch, numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from torchmetrics.classification import MultilabelF1Score, MultilabelAccuracy\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# ──────────────── 1. DATASET & DATALOADER ────────────────\n",
        "class CAERDataset(Dataset):\n",
        "    def __init__(self, pkl_path):\n",
        "        with open(pkl_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self.data = data\n",
        "        self.ids  = list(data['CMU_MOSEI_Labels'].keys())\n",
        "\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        k = self.ids[idx]\n",
        "        t = torch.as_tensor(self.data['CMU_MOSEI_TimestampedWordVectors'][k]['features'], dtype=torch.float32)\n",
        "        a = torch.as_tensor(self.data['CMU_MOSEI_COVAREP'][k]['features'], dtype=torch.float32)\n",
        "        v = torch.as_tensor(self.data['CMU_MOSEI_VisualFacet42'][k]['features'], dtype=torch.float32)\n",
        "        y = torch.as_tensor(self.data['CMU_MOSEI_Labels'][k]['features'], dtype=torch.float32)\n",
        "\n",
        "        # Patch: avoid empty sequences\n",
        "        if t.size(0) == 0: t = torch.zeros(1, 300)\n",
        "        if a.size(0) == 0: a = torch.zeros(1, 74)\n",
        "        if v.size(0) == 0: v = torch.zeros(1, 35)\n",
        "\n",
        "        return t, a, v, y\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    ts, as_, vs, ys = zip(*batch)\n",
        "    t_p = pad_sequence(ts, batch_first=True)\n",
        "    t_l = torch.tensor([x.size(0) for x in ts])\n",
        "    a_p = pad_sequence(as_, batch_first=True)\n",
        "    a_l = torch.tensor([x.size(0) for x in as_])\n",
        "    v_p = pad_sequence(vs, batch_first=True)\n",
        "    v_l = torch.tensor([x.size(0) for x in vs])\n",
        "    y   = torch.stack(ys)\n",
        "    return t_p, t_l, a_p, a_l, v_p, v_l, y\n",
        "\n",
        "# load & split\n",
        "DATA_PKL = \"/content/drive/MyDrive/caer_aligned_mosei_format.pkl\"\n",
        "dataset  = CAERDataset(DATA_PKL)\n",
        "N        = len(dataset)\n",
        "train_n  = int(0.7*N)\n",
        "val_n    = int(0.15*N)\n",
        "test_n   = N - train_n - val_n\n",
        "train_ds, val_ds, test_ds = random_split(dataset, [train_n, val_n, test_n], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  collate_fn=collate_fn, num_workers=4)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
        "\n",
        "# ──────────────── 2. SCRATCH MODEL ────────────────\n",
        "class ScratchXModal(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.text_rnn   = nn.GRU(300,256,batch_first=True)\n",
        "        self.audio_rnn  = nn.GRU( 74,128,batch_first=True)\n",
        "        self.visual_rnn = nn.GRU( 35,128,batch_first=True)\n",
        "        self.fc         = nn.Linear(256+128+128, 7)\n",
        "        self.loss       = nn.BCEWithLogitsLoss()\n",
        "        self.thresh     = 0.3\n",
        "        self.f1         = MultilabelF1Score(num_labels=7, threshold=self.thresh, average='macro')\n",
        "        self.acc        = MultilabelAccuracy(num_labels=7, threshold=self.thresh)\n",
        "\n",
        "    def forward(self, t, tl, a, al, v, vl):\n",
        "        def last(rnn, x, lengths):\n",
        "            _, h = rnn(pack_padded_sequence(x, lengths.cpu(),\n",
        "                                            batch_first=True,\n",
        "                                            enforce_sorted=False))\n",
        "            return h.squeeze(0)\n",
        "        h_t = last(self.text_rnn,   t,  tl)\n",
        "        h_a = last(self.audio_rnn,  a,  al)\n",
        "        h_v = last(self.visual_rnn, v,  vl)\n",
        "        return self.fc(torch.cat([h_t,h_a,h_v], dim=-1))\n",
        "\n",
        "    def training_step(self, batch, _):\n",
        "        t, tl, a, al, v, vl, y = batch\n",
        "        logits = self(t,tl,a,al,v,vl)\n",
        "        loss   = self.loss(logits, y)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, _):\n",
        "        t, tl, a, al, v, vl, y = batch\n",
        "        logits = self(t,tl,a,al,v,vl)\n",
        "        preds  = (torch.sigmoid(logits)>self.thresh).int()\n",
        "        self.f1.update(preds, y.int())\n",
        "        self.acc.update(preds, y.int())\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        self.log(\"val_macro_f1\", self.f1.compute(), prog_bar=True)\n",
        "        self.log(\"val_acc\",      self.acc.compute(), prog_bar=True)\n",
        "        self.f1.reset(); self.acc.reset()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
        "\n",
        "# ──────────────── 3. TRAIN & SAVE ────────────────\n",
        "ckpt_cb = ModelCheckpoint(\n",
        "    monitor=\"val_macro_f1\", mode=\"max\",\n",
        "    filename=\"scratch-caer-{epoch:02d}-{val_macro_f1:.3f}\",\n",
        "    save_top_k=1\n",
        ")\n",
        "stop_cb = EarlyStopping(monitor=\"val_macro_f1\", mode=\"max\", patience=5)\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=100,\n",
        "    accelerator=\"auto\",\n",
        "    callbacks=[ckpt_cb, stop_cb]\n",
        ")\n",
        "model = ScratchXModal()\n",
        "trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "print(\"Best Scratch-CAER ckpt:\", ckpt_cb.best_model_path)\n"
      ],
      "metadata": {
        "id": "wzB9SyzjPDAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics.classification import (\n",
        "    MultilabelF1Score,\n",
        "    MultilabelPrecision,\n",
        "    MultilabelRecall,\n",
        "    MultilabelAccuracy\n",
        ")\n",
        "\n",
        "# ---------- Dataset ----------\n",
        "class CAERMoseiFormatDataset(Dataset):\n",
        "    def __init__(self, pkl_path):\n",
        "        with open(pkl_path, 'rb') as f:\n",
        "            self.data = pickle.load(f)\n",
        "        self.ids = list(self.data[\"CMU_MOSEI_Labels\"].keys())\n",
        "\n",
        "    def __len__(self): return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        k = self.ids[idx]\n",
        "        def get(name): return torch.tensor(self.data[name][k][\"features\"], dtype=torch.float32)\n",
        "        return get(\"CMU_MOSEI_TimestampedWordVectors\"), get(\"CMU_MOSEI_COVAREP\"), get(\"CMU_MOSEI_VisualFacet42\"), torch.tensor(self.data[\"CMU_MOSEI_Labels\"][k][\"features\"], dtype=torch.float32)\n",
        "\n",
        "def collate(batch):\n",
        "    ts, as_, vs, ys = zip(*batch)\n",
        "    t_p = pad_sequence(ts, batch_first=True)\n",
        "    t_l = torch.tensor([x.size(0) for x in ts])\n",
        "    a_p = pad_sequence(as_, batch_first=True)\n",
        "    a_l = torch.tensor([x.size(0) for x in as_])\n",
        "    v_p = pad_sequence(vs, batch_first=True)\n",
        "    v_l = torch.tensor([x.size(0) for x in vs])\n",
        "    y   = torch.stack(ys)\n",
        "    return t_p, t_l, a_p, a_l, v_p, v_l, y\n",
        "\n",
        "# ---------- Model ----------\n",
        "class XModal(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.text_rnn   = nn.GRU(300, 256, batch_first=True)\n",
        "        self.audio_rnn  = nn.GRU(74, 128, batch_first=True)\n",
        "        self.visual_rnn = nn.GRU(35, 128, batch_first=True)\n",
        "        self.fc         = nn.Linear(256 + 128 + 128, 7)\n",
        "\n",
        "    def forward(self, t, tl, a, al, v, vl):\n",
        "        def last(rnn, x, lengths):\n",
        "            packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "            return rnn(packed)[1].squeeze(0)\n",
        "        h_t = last(self.text_rnn,   t, tl)\n",
        "        h_a = last(self.audio_rnn,  a, al)\n",
        "        h_v = last(self.visual_rnn, v, vl)\n",
        "        return self.fc(torch.cat([h_t, h_a, h_v], dim=-1))\n",
        "\n",
        "# ---------- Evaluation ----------\n",
        "if __name__ == \"__main__\":\n",
        "    pkl_path  = \"/content/drive/MyDrive/caer_aligned_mosei_format.pkl\"\n",
        "    ckpt_path = \"/content/drive/MyDrive/EmotionFusion/checkpoints/best-epoch=12-val_f1_micro=0.459.ckpt\"\n",
        "\n",
        "    ds = CAERMoseiFormatDataset(pkl_path)\n",
        "    N = len(ds)\n",
        "    train_sz = int(0.7 * N)\n",
        "    val_sz   = int(0.15 * N)\n",
        "    test_sz  = N - train_sz - val_sz  # ensures exact sum\n",
        "    _, _, test_ds = random_split(ds, [train_sz, val_sz, test_sz])\n",
        "    test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=collate)\n",
        "\n",
        "    model = XModal.load_from_checkpoint(ckpt_path, strict=False)\n",
        "    model.eval().cuda()\n",
        "\n",
        "    # Metrics\n",
        "    threshold = 0.3\n",
        "    f1    = MultilabelF1Score(num_labels=7, threshold=threshold, average='none').cuda()\n",
        "    prec  = MultilabelPrecision(num_labels=7, threshold=threshold, average='none').cuda()\n",
        "    rec   = MultilabelRecall(num_labels=7, threshold=threshold, average='none').cuda()\n",
        "    acc   = MultilabelAccuracy(num_labels=7, threshold=threshold, average='none').cuda()\n",
        "\n",
        "    # Run\n",
        "    print(\"🔍 Evaluating Zero-Shot MOSEI model on CAER...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            t, tl, a, al, v, vl, y = [x.cuda() for x in batch]\n",
        "            logits = model(t, tl, a, al, v, vl)\n",
        "            preds  = (torch.sigmoid(logits) > threshold).int()\n",
        "            f1.update(preds, y.int())\n",
        "            prec.update(preds, y.int())\n",
        "            rec.update(preds, y.int())\n",
        "            acc.update(preds, y.int())\n",
        "\n",
        "    # Results\n",
        "    import pandas as pd\n",
        "    class_names = [\"happy\", \"sad\", \"angry\", \"surprise\", \"disgust\", \"fear\", \"neutral\"]\n",
        "    df = pd.DataFrame({\n",
        "        \"Emotion\":   class_names,\n",
        "        \"F1\":        f1.compute().cpu().numpy(),\n",
        "        \"Precision\": prec.compute().cpu().numpy(),\n",
        "        \"Recall\":    rec.compute().cpu().numpy(),\n",
        "        \"Accuracy\":  acc.compute().cpu().numpy()\n",
        "    })\n",
        "    print(df.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "7pw6P78bnYVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics.classification import (\n",
        "    MultilabelF1Score,\n",
        "    MultilabelPrecision,\n",
        "    MultilabelRecall,\n",
        "    MultilabelAccuracy\n",
        ")\n",
        "\n",
        "# ---------------------- CAER Dataset ----------------------\n",
        "class CAERDataset(Dataset):\n",
        "    def __init__(self, pkl_path):\n",
        "        with open(pkl_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self.data = data\n",
        "        self.ids  = list(data['CMU_MOSEI_Labels'].keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        k = self.ids[idx]\n",
        "        t = torch.as_tensor(self.data['CMU_MOSEI_TimestampedWordVectors'][k]['features'])\n",
        "        a = torch.as_tensor(self.data['CMU_MOSEI_COVAREP'][k]['features'])\n",
        "        v = torch.as_tensor(self.data['CMU_MOSEI_VisualFacet42'][k]['features'])\n",
        "        y = torch.as_tensor(self.data['CMU_MOSEI_Labels'][k]['features'])\n",
        "        return t, a, v, y\n",
        "\n",
        "def coll(batch):\n",
        "    ts, as_, vs, ys = zip(*batch)\n",
        "    t = torch.stack(ts).unsqueeze(1)                 # (B,1,300)\n",
        "    tlen = torch.ones(len(ts), dtype=torch.long)     # all length=1\n",
        "\n",
        "    alens = torch.tensor([a.shape[0] for a in as_])\n",
        "    a_padded = pad_sequence(as_, batch_first=True)   # (B, T_a, 65)\n",
        "\n",
        "    vlens = torch.tensor([v.shape[0] for v in vs])\n",
        "    v_padded = pad_sequence(vs, batch_first=True)    # (B, T_v, 2048)\n",
        "\n",
        "    y = torch.stack(ys)                              # (B,7)\n",
        "    return t, tlen, a_padded, alens, v_padded, vlens, y\n",
        "\n",
        "# ---------------------- Fine-tunable Model Definition ----------------------\n",
        "class XModalProj(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define MOSEI‐pretrained architecture slots to be filled by checkpoint\n",
        "        self.text_rnn   = nn.GRU(300, 256, batch_first=True)\n",
        "        self.audio_rnn  = nn.GRU(74,  128, batch_first=True)\n",
        "        self.visual_rnn = nn.GRU(35,  128, batch_first=True)\n",
        "        self.fc         = nn.Linear(256 + 128 + 128, 7)\n",
        "\n",
        "        # Projection layers for CAER→MOSEI dims\n",
        "        self.a_proj = nn.Linear(65,   74, bias=False)\n",
        "        self.v_proj = nn.Linear(2048, 35, bias=True)\n",
        "        nn.init.eye_( self.a_proj.weight )\n",
        "        nn.init.zeros_(self.v_proj.weight)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.loss    = nn.BCEWithLogitsLoss()\n",
        "        self.thresh  = 0.3\n",
        "\n",
        "    def forward(self, t, tlen, a, alen, v, vlen):\n",
        "        # project CAER features into MOSEI dims\n",
        "        a_proj = self.a_proj(a)\n",
        "        v_proj = self.v_proj(v)\n",
        "\n",
        "        # helper to get last hidden state\n",
        "        def last(rnn, x, lengths):\n",
        "            packed = pack_padded_sequence(x, lengths.cpu(),\n",
        "                                          batch_first=True,\n",
        "                                          enforce_sorted=False)\n",
        "            return rnn(packed)[1].squeeze(0)\n",
        "\n",
        "        h_text  = last(self.text_rnn,   t,      tlen)\n",
        "        h_audio = last(self.audio_rnn,  a_proj, alen)\n",
        "        h_vis   = last(self.visual_rnn, v_proj, vlen)\n",
        "        h       = torch.cat([h_text, h_audio, h_vis], dim=-1)\n",
        "        h       = self.dropout(h)\n",
        "        return self.fc(h)\n",
        "\n",
        "# ---------------------- Evaluation Script ----------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Paths\n",
        "    DRIVE_ROOT     = \"/content/drive/MyDrive\"\n",
        "    PICKLE_PATH    = f\"{DRIVE_ROOT}/caer_aligned_full.pkl\"\n",
        "    BEST_CKPT_PATH = f\"{DRIVE_ROOT}/CAER_finetune_checkpoints/best-caer-epoch=25-val_f1=0.391.ckpt\"\n",
        "\n",
        "    # Load dataset and split\n",
        "    ds = CAERDataset(PICKLE_PATH)\n",
        "    N  = len(ds)\n",
        "    train_sz = int(0.7 * N)\n",
        "    val_sz   = int(0.15 * N)\n",
        "    test_sz  = N - train_sz - val_sz\n",
        "    _, _, test_ds = random_split(ds, [train_sz, val_sz, test_sz])\n",
        "    test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=coll)\n",
        "\n",
        "    # Load model from checkpoint (fills text_rnn, audio_rnn, visual_rnn, fc)\n",
        "    model = XModalProj.load_from_checkpoint(BEST_CKPT_PATH, strict=False)\n",
        "    model.eval().cuda()\n",
        "\n",
        "    # Metrics\n",
        "    threshold = 0.3\n",
        "    f1    = MultilabelF1Score(num_labels=7, threshold=threshold, average='none').cuda()\n",
        "    prec  = MultilabelPrecision(num_labels=7, threshold=threshold, average='none').cuda()\n",
        "    rec   = MultilabelRecall(num_labels=7, threshold=threshold, average='none').cuda()\n",
        "    acc   = MultilabelAccuracy(num_labels=7, threshold=threshold, average='none').cuda()\n",
        "\n",
        "    # Run evaluation\n",
        "    print(\"🔍 Evaluating on CAER test set...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            t, tlen, a, alen, v, vlen, y = [x.cuda() for x in batch]\n",
        "            logits = model(t, tlen, a, alen, v, vlen)\n",
        "            preds  = (torch.sigmoid(logits) > threshold).int()\n",
        "            y_bin  = y.int().cuda()\n",
        "            f1.update(preds, y_bin)\n",
        "            prec.update(preds, y_bin)\n",
        "            rec.update(preds, y_bin)\n",
        "            acc.update(preds, y_bin)\n",
        "\n",
        "    # Report results\n",
        "    class_names = [\"happy\",\"sad\",\"angry\",\"surprise\",\"disgust\",\"fear\",\"neutral\"]\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame({\n",
        "        \"emotion\":   class_names,\n",
        "        \"F1\":        f1.compute().cpu().numpy(),\n",
        "        \"Precision\": prec.compute().cpu().numpy(),\n",
        "        \"Recall\":    rec.compute().cpu().numpy(),\n",
        "        \"Accuracy\":  acc.compute().cpu().numpy()\n",
        "    })\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "import pickle\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics.classification import (\n",
        "    MultilabelF1Score,\n",
        "    MultilabelPrecision,\n",
        "    MultilabelRecall,\n",
        "    MultilabelAccuracy\n",
        ")\n",
        "import pandas as pd\n",
        "\n",
        "# ---------------------- Dataset ----------------------\n",
        "class CAERScratchDataset(Dataset):\n",
        "    def __init__(self, pkl_path):\n",
        "        with open(pkl_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self.data = data\n",
        "        self.ids  = list(data['CMU_MOSEI_Labels'].keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        k = self.ids[idx]\n",
        "        t = torch.as_tensor(self.data['CMU_MOSEI_TimestampedWordVectors'][k]['features'])\n",
        "        a = torch.as_tensor(self.data['CMU_MOSEI_COVAREP'][k]['features'])\n",
        "        v = torch.as_tensor(self.data['CMU_MOSEI_VisualFacet42'][k]['features'])\n",
        "        y = torch.as_tensor(self.data['CMU_MOSEI_Labels'][k]['features'])\n",
        "        return t, a, v, y\n",
        "\n",
        "def coll(batch):\n",
        "    ts, as_, vs, ys = zip(*batch)\n",
        "    t = pad_sequence(ts, batch_first=True)\n",
        "    tlen = torch.tensor([x.size(0) for x in ts])\n",
        "    a_p = pad_sequence(as_, batch_first=True)\n",
        "    alen = torch.tensor([x.size(0) for x in as_])\n",
        "    v_p = pad_sequence(vs, batch_first=True)\n",
        "    vlen = torch.tensor([x.size(0) for x in vs])\n",
        "    y = torch.stack(ys)\n",
        "    return t, tlen, a_p, alen, v_p, vlen, y\n",
        "\n",
        "# ---------------------- Scratch Model ----------------------\n",
        "class ScratchXModal(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.text_rnn   = nn.GRU(300, 256, batch_first=True)\n",
        "        self.audio_rnn  = nn.GRU(74,  128, batch_first=True)\n",
        "        self.visual_rnn = nn.GRU(35,  128, batch_first=True)\n",
        "        self.fc         = nn.Linear(256 + 128 + 128, 7)\n",
        "        self.dropout    = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, t, tlen, a, alen, v, vlen):\n",
        "        def last(rnn, x, lengths):\n",
        "            packed = pack_padded_sequence(x, lengths.cpu(),\n",
        "                                          batch_first=True,\n",
        "                                          enforce_sorted=False)\n",
        "            return rnn(packed)[1].squeeze(0)\n",
        "\n",
        "        h_t = last(self.text_rnn,   t,  tlen)\n",
        "        h_a = last(self.audio_rnn,  a,  alen)\n",
        "        h_v = last(self.visual_rnn, v,  vlen)\n",
        "        h   = torch.cat([h_t, h_a, h_v], dim=-1)\n",
        "        h   = self.dropout(h)\n",
        "        return self.fc(h)\n",
        "\n",
        "# ---------------------- Evaluation ----------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pkl_path  = \"/content/drive/MyDrive/caer_aligned_mosei_format.pkl\"\n",
        "ckpt_path = \"/content/drive/MyDrive/scratch-caer-epoch=33-val_macro_f1=0.134.ckpt\"\n",
        "\n",
        "# Load dataset\n",
        "ds = CAERScratchDataset(pkl_path)\n",
        "N = len(ds)\n",
        "train_sz, val_sz = int(0.7 * N), int(0.15 * N)\n",
        "test_sz = N - train_sz - val_sz\n",
        "_, _, test_ds = random_split(ds, [train_sz, val_sz, test_sz])\n",
        "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=coll)\n",
        "\n",
        "# Load model\n",
        "model = ScratchXModal.load_from_checkpoint(ckpt_path, strict=False)\n",
        "model.eval().to(device)\n",
        "\n",
        "# Metrics\n",
        "threshold = 0.3\n",
        "f1    = MultilabelF1Score(num_labels=7, threshold=threshold, average='none').to(device)\n",
        "prec  = MultilabelPrecision(num_labels=7, threshold=threshold, average='none').to(device)\n",
        "rec   = MultilabelRecall(num_labels=7, threshold=threshold, average='none').to(device)\n",
        "acc   = MultilabelAccuracy(num_labels=7, threshold=threshold, average='none').to(device)\n",
        "\n",
        "# Evaluate\n",
        "print(\"🔍 Evaluating Scratch-CAER model on test set...\")\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        t, tlen, a, alen, v, vlen, y = [x.to(device) for x in batch]\n",
        "        logits = model(t, tlen, a, alen, v, vlen)\n",
        "        preds  = (torch.sigmoid(logits) > threshold).int()\n",
        "        f1.update(preds, y.int())\n",
        "        prec.update(preds, y.int())\n",
        "        rec.update(preds, y.int())\n",
        "        acc.update(preds, y.int())\n",
        "\n",
        "# Report\n",
        "class_names = [\"happy\", \"sad\", \"angry\", \"surprise\", \"disgust\", \"fear\", \"neutral\"]\n",
        "df = pd.DataFrame({\n",
        "    \"emotion\":   class_names,\n",
        "    \"F1\":        f1.compute().cpu().numpy(),\n",
        "    \"Precision\": prec.compute().cpu().numpy(),\n",
        "    \"Recall\":    rec.compute().cpu().numpy(),\n",
        "    \"Accuracy\":  acc.compute().cpu().numpy()\n",
        "})\n",
        "print(df.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "33oJz7ZJXVbd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}